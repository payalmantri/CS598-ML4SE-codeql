[{"repo": "ageitgey/face_recognition", "path": "examples/face_recognition_knn.py", "func_name": "train", "original_string": "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    \"\"\"\n    Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        \u251c\u2500\u2500 <person1>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u251c\u2500\u2500 <somename2>.jpeg\n        \u2502   \u251c\u2500\u2500 ...\n        \u251c\u2500\u2500 <person2>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u2514\u2500\u2500 <somename2>.jpeg\n        \u2514\u2500\u2500 ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.\n    \"\"\"\n    X = []\n    y = []\n\n    # Loop through each person in the training set\n    for class_dir in os.listdir(train_dir):\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n            continue\n\n        # Loop through each training image for the current person\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n\n            if len(face_bounding_boxes) != 1:\n                # If there are no people (or too many people) in a training image, skip the image.\n                if verbose:\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n            else:\n                # Add face encoding for current image to the training set\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n                y.append(class_dir)\n\n    # Determine how many neighbors to use for weighting in the KNN classifier\n    if n_neighbors is None:\n        n_neighbors = int(round(math.sqrt(len(X))))\n        if verbose:\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\n\n    # Create and train the KNN classifier\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n    knn_clf.fit(X, y)\n\n    # Save the trained KNN classifier\n    if model_save_path is not None:\n        with open(model_save_path, 'wb') as f:\n            pickle.dump(knn_clf, f)\n\n    return knn_clf", "language": "python", "code": "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    \"\"\"\n    Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        \u251c\u2500\u2500 <person1>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u251c\u2500\u2500 <somename2>.jpeg\n        \u2502   \u251c\u2500\u2500 ...\n        \u251c\u2500\u2500 <person2>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u2514\u2500\u2500 <somename2>.jpeg\n        \u2514\u2500\u2500 ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.\n    \"\"\"\n    X = []\n    y = []\n\n    # Loop through each person in the training set\n    for class_dir in os.listdir(train_dir):\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n            continue\n\n        # Loop through each training image for the current person\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n\n            if len(face_bounding_boxes) != 1:\n                # If there are no people (or too many people) in a training image, skip the image.\n                if verbose:\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n            else:\n                # Add face encoding for current image to the training set\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n                y.append(class_dir)\n\n    # Determine how many neighbors to use for weighting in the KNN classifier\n    if n_neighbors is None:\n        n_neighbors = int(round(math.sqrt(len(X))))\n        if verbose:\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\n\n    # Create and train the KNN classifier\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n    knn_clf.fit(X, y)\n\n    # Save the trained KNN classifier\n    if model_save_path is not None:\n        with open(model_save_path, 'wb') as f:\n            pickle.dump(knn_clf, f)\n\n    return knn_clf", "code_tokens": ["def", "train", "(", "train_dir", ",", "model_save_path", "=", "None", ",", "n_neighbors", "=", "None", ",", "knn_algo", "=", "'ball_tree'", ",", "verbose", "=", "False", ")", ":", "X", "=", "[", "]", "y", "=", "[", "]", "# Loop through each person in the training set", "for", "class_dir", "in", "os", ".", "listdir", "(", "train_dir", ")", ":", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "train_dir", ",", "class_dir", ")", ")", ":", "continue", "# Loop through each training image for the current person", "for", "img_path", "in", "image_files_in_folder", "(", "os", ".", "path", ".", "join", "(", "train_dir", ",", "class_dir", ")", ")", ":", "image", "=", "face_recognition", ".", "load_image_file", "(", "img_path", ")", "face_bounding_boxes", "=", "face_recognition", ".", "face_locations", "(", "image", ")", "if", "len", "(", "face_bounding_boxes", ")", "!=", "1", ":", "# If there are no people (or too many people) in a training image, skip the image.", "if", "verbose", ":", "print", "(", "\"Image {} not suitable for training: {}\"", ".", "format", "(", "img_path", ",", "\"Didn't find a face\"", "if", "len", "(", "face_bounding_boxes", ")", "<", "1", "else", "\"Found more than one face\"", ")", ")", "else", ":", "# Add face encoding for current image to the training set", "X", ".", "append", "(", "face_recognition", ".", "face_encodings", "(", "image", ",", "known_face_locations", "=", "face_bounding_boxes", ")", "[", "0", "]", ")", "y", ".", "append", "(", "class_dir", ")", "# Determine how many neighbors to use for weighting in the KNN classifier", "if", "n_neighbors", "is", "None", ":", "n_neighbors", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "len", "(", "X", ")", ")", ")", ")", "if", "verbose", ":", "print", "(", "\"Chose n_neighbors automatically:\"", ",", "n_neighbors", ")", "# Create and train the KNN classifier", "knn_clf", "=", "neighbors", ".", "KNeighborsClassifier", "(", "n_neighbors", "=", "n_neighbors", ",", "algorithm", "=", "knn_algo", ",", "weights", "=", "'distance'", ")", "knn_clf", ".", "fit", "(", "X", ",", "y", ")", "# Save the trained KNN classifier", "if", "model_save_path", "is", "not", "None", ":", "with", "open", "(", "model_save_path", ",", "'wb'", ")", "as", "f", ":", "pickle", ".", "dump", "(", "knn_clf", ",", "f", ")", "return", "knn_clf"], "docstring": "Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        \u251c\u2500\u2500 <person1>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u251c\u2500\u2500 <somename2>.jpeg\n        \u2502   \u251c\u2500\u2500 ...\n        \u251c\u2500\u2500 <person2>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u2514\u2500\u2500 <somename2>.jpeg\n        \u2514\u2500\u2500 ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.", "docstring_tokens": ["Trains", "a", "k", "-", "nearest", "neighbors", "classifier", "for", "face", "recognition", "."], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "examples/face_recognition_knn.py", "func_name": "predict", "original_string": "def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n    \"\"\"\n    Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.\n    \"\"\"\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n\n    if knn_clf is None and model_path is None:\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n\n    # Load a trained KNN model (if one was passed in)\n    if knn_clf is None:\n        with open(model_path, 'rb') as f:\n            knn_clf = pickle.load(f)\n\n    # Load image file and find face locations\n    X_img = face_recognition.load_image_file(X_img_path)\n    X_face_locations = face_recognition.face_locations(X_img)\n\n    # If no faces are found in the image, return an empty result.\n    if len(X_face_locations) == 0:\n        return []\n\n    # Find encodings for faces in the test iamge\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n\n    # Use the KNN model to find the best matches for the test face\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n\n    # Predict classes and remove classifications that aren't within the threshold\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]", "language": "python", "code": "def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n    \"\"\"\n    Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.\n    \"\"\"\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n\n    if knn_clf is None and model_path is None:\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n\n    # Load a trained KNN model (if one was passed in)\n    if knn_clf is None:\n        with open(model_path, 'rb') as f:\n            knn_clf = pickle.load(f)\n\n    # Load image file and find face locations\n    X_img = face_recognition.load_image_file(X_img_path)\n    X_face_locations = face_recognition.face_locations(X_img)\n\n    # If no faces are found in the image, return an empty result.\n    if len(X_face_locations) == 0:\n        return []\n\n    # Find encodings for faces in the test iamge\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n\n    # Use the KNN model to find the best matches for the test face\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n\n    # Predict classes and remove classifications that aren't within the threshold\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]", "code_tokens": ["def", "predict", "(", "X_img_path", ",", "knn_clf", "=", "None", ",", "model_path", "=", "None", ",", "distance_threshold", "=", "0.6", ")", ":", "if", "not", "os", ".", "path", ".", "isfile", "(", "X_img_path", ")", "or", "os", ".", "path", ".", "splitext", "(", "X_img_path", ")", "[", "1", "]", "[", "1", ":", "]", "not", "in", "ALLOWED_EXTENSIONS", ":", "raise", "Exception", "(", "\"Invalid image path: {}\"", ".", "format", "(", "X_img_path", ")", ")", "if", "knn_clf", "is", "None", "and", "model_path", "is", "None", ":", "raise", "Exception", "(", "\"Must supply knn classifier either thourgh knn_clf or model_path\"", ")", "# Load a trained KNN model (if one was passed in)", "if", "knn_clf", "is", "None", ":", "with", "open", "(", "model_path", ",", "'rb'", ")", "as", "f", ":", "knn_clf", "=", "pickle", ".", "load", "(", "f", ")", "# Load image file and find face locations", "X_img", "=", "face_recognition", ".", "load_image_file", "(", "X_img_path", ")", "X_face_locations", "=", "face_recognition", ".", "face_locations", "(", "X_img", ")", "# If no faces are found in the image, return an empty result.", "if", "len", "(", "X_face_locations", ")", "==", "0", ":", "return", "[", "]", "# Find encodings for faces in the test iamge", "faces_encodings", "=", "face_recognition", ".", "face_encodings", "(", "X_img", ",", "known_face_locations", "=", "X_face_locations", ")", "# Use the KNN model to find the best matches for the test face", "closest_distances", "=", "knn_clf", ".", "kneighbors", "(", "faces_encodings", ",", "n_neighbors", "=", "1", ")", "are_matches", "=", "[", "closest_distances", "[", "0", "]", "[", "i", "]", "[", "0", "]", "<=", "distance_threshold", "for", "i", "in", "range", "(", "len", "(", "X_face_locations", ")", ")", "]", "# Predict classes and remove classifications that aren't within the threshold", "return", "[", "(", "pred", ",", "loc", ")", "if", "rec", "else", "(", "\"unknown\"", ",", "loc", ")", "for", "pred", ",", "loc", ",", "rec", "in", "zip", "(", "knn_clf", ".", "predict", "(", "faces_encodings", ")", ",", "X_face_locations", ",", "are_matches", ")", "]"], "docstring": "Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.", "docstring_tokens": ["Recognizes", "faces", "in", "given", "image", "using", "a", "trained", "KNN", "classifier"], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L111-L150", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "examples/face_recognition_knn.py", "func_name": "show_prediction_labels_on_image", "original_string": "def show_prediction_labels_on_image(img_path, predictions):\n    \"\"\"\n    Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:\n    \"\"\"\n    pil_image = Image.open(img_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(pil_image)\n\n    for name, (top, right, bottom, left) in predictions:\n        # Draw a box around the face using the Pillow module\n        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n        # There's a bug in Pillow where it blows up with non-UTF-8 text\n        # when using the default bitmap font\n        name = name.encode(\"UTF-8\")\n\n        # Draw a label with a name below the face\n        text_width, text_height = draw.textsize(name)\n        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n    # Remove the drawing library from memory as per the Pillow docs\n    del draw\n\n    # Display the resulting image\n    pil_image.show()", "language": "python", "code": "def show_prediction_labels_on_image(img_path, predictions):\n    \"\"\"\n    Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:\n    \"\"\"\n    pil_image = Image.open(img_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(pil_image)\n\n    for name, (top, right, bottom, left) in predictions:\n        # Draw a box around the face using the Pillow module\n        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n        # There's a bug in Pillow where it blows up with non-UTF-8 text\n        # when using the default bitmap font\n        name = name.encode(\"UTF-8\")\n\n        # Draw a label with a name below the face\n        text_width, text_height = draw.textsize(name)\n        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n    # Remove the drawing library from memory as per the Pillow docs\n    del draw\n\n    # Display the resulting image\n    pil_image.show()", "code_tokens": ["def", "show_prediction_labels_on_image", "(", "img_path", ",", "predictions", ")", ":", "pil_image", "=", "Image", ".", "open", "(", "img_path", ")", ".", "convert", "(", "\"RGB\"", ")", "draw", "=", "ImageDraw", ".", "Draw", "(", "pil_image", ")", "for", "name", ",", "(", "top", ",", "right", ",", "bottom", ",", "left", ")", "in", "predictions", ":", "# Draw a box around the face using the Pillow module", "draw", ".", "rectangle", "(", "(", "(", "left", ",", "top", ")", ",", "(", "right", ",", "bottom", ")", ")", ",", "outline", "=", "(", "0", ",", "0", ",", "255", ")", ")", "# There's a bug in Pillow where it blows up with non-UTF-8 text", "# when using the default bitmap font", "name", "=", "name", ".", "encode", "(", "\"UTF-8\"", ")", "# Draw a label with a name below the face", "text_width", ",", "text_height", "=", "draw", ".", "textsize", "(", "name", ")", "draw", ".", "rectangle", "(", "(", "(", "left", ",", "bottom", "-", "text_height", "-", "10", ")", ",", "(", "right", ",", "bottom", ")", ")", ",", "fill", "=", "(", "0", ",", "0", ",", "255", ")", ",", "outline", "=", "(", "0", ",", "0", ",", "255", ")", ")", "draw", ".", "text", "(", "(", "left", "+", "6", ",", "bottom", "-", "text_height", "-", "5", ")", ",", "name", ",", "fill", "=", "(", "255", ",", "255", ",", "255", ",", "255", ")", ")", "# Remove the drawing library from memory as per the Pillow docs", "del", "draw", "# Display the resulting image", "pil_image", ".", "show", "(", ")"], "docstring": "Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:", "docstring_tokens": ["Shows", "the", "face", "recognition", "results", "visually", "."], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L153-L181", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "_rect_to_css", "original_string": "def _rect_to_css(rect):\n    \"\"\"\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return rect.top(), rect.right(), rect.bottom(), rect.left()", "language": "python", "code": "def _rect_to_css(rect):\n    \"\"\"\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return rect.top(), rect.right(), rect.bottom(), rect.left()", "code_tokens": ["def", "_rect_to_css", "(", "rect", ")", ":", "return", "rect", ".", "top", "(", ")", ",", "rect", ".", "right", "(", ")", ",", "rect", ".", "bottom", "(", ")", ",", "rect", ".", "left", "(", ")"], "docstring": "Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order", "docstring_tokens": ["Convert", "a", "dlib", "rect", "object", "to", "a", "plain", "tuple", "in", "(", "top", "right", "bottom", "left", ")", "order"], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L32-L39", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "_trim_css_to_bounds", "original_string": "def _trim_css_to_bounds(css, image_shape):\n    \"\"\"\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)", "language": "python", "code": "def _trim_css_to_bounds(css, image_shape):\n    \"\"\"\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)", "code_tokens": ["def", "_trim_css_to_bounds", "(", "css", ",", "image_shape", ")", ":", "return", "max", "(", "css", "[", "0", "]", ",", "0", ")", ",", "min", "(", "css", "[", "1", "]", ",", "image_shape", "[", "1", "]", ")", ",", "min", "(", "css", "[", "2", "]", ",", "image_shape", "[", "0", "]", ")", ",", "max", "(", "css", "[", "3", "]", ",", "0", ")"], "docstring": "Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order", "docstring_tokens": ["Make", "sure", "a", "tuple", "in", "(", "top", "right", "bottom", "left", ")", "order", "is", "within", "the", "bounds", "of", "the", "image", "."], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L52-L60", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "face_distance", "original_string": "def face_distance(face_encodings, face_to_compare):\n    \"\"\"\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n    \"\"\"\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)", "language": "python", "code": "def face_distance(face_encodings, face_to_compare):\n    \"\"\"\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n    \"\"\"\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)", "code_tokens": ["def", "face_distance", "(", "face_encodings", ",", "face_to_compare", ")", ":", "if", "len", "(", "face_encodings", ")", "==", "0", ":", "return", "np", ".", "empty", "(", "(", "0", ")", ")", "return", "np", ".", "linalg", ".", "norm", "(", "face_encodings", "-", "face_to_compare", ",", "axis", "=", "1", ")"], "docstring": "Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array", "docstring_tokens": ["Given", "a", "list", "of", "face", "encodings", "compare", "them", "to", "a", "known", "face", "encoding", "and", "get", "a", "euclidean", "distance", "for", "each", "comparison", "face", ".", "The", "distance", "tells", "you", "how", "similar", "the", "faces", "are", "."], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L63-L75", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "load_image_file", "original_string": "def load_image_file(file, mode='RGB'):\n    \"\"\"\n    Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array\n    \"\"\"\n    im = PIL.Image.open(file)\n    if mode:\n        im = im.convert(mode)\n    return np.array(im)", "language": "python", "code": "def load_image_file(file, mode='RGB'):\n    \"\"\"\n    Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array\n    \"\"\"\n    im = PIL.Image.open(file)\n    if mode:\n        im = im.convert(mode)\n    return np.array(im)", "code_tokens": ["def", "load_image_file", "(", "file", ",", "mode", "=", "'RGB'", ")", ":", "im", "=", "PIL", ".", "Image", ".", "open", "(", "file", ")", "if", "mode", ":", "im", "=", "im", ".", "convert", "(", "mode", ")", "return", "np", ".", "array", "(", "im", ")"], "docstring": "Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array", "docstring_tokens": ["Loads", "an", "image", "file", "(", ".", "jpg", ".", "png", "etc", ")", "into", "a", "numpy", "array"], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L78-L89", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "_raw_face_locations", "original_string": "def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations\n    \"\"\"\n    if model == \"cnn\":\n        return cnn_face_detector(img, number_of_times_to_upsample)\n    else:\n        return face_detector(img, number_of_times_to_upsample)", "language": "python", "code": "def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations\n    \"\"\"\n    if model == \"cnn\":\n        return cnn_face_detector(img, number_of_times_to_upsample)\n    else:\n        return face_detector(img, number_of_times_to_upsample)", "code_tokens": ["def", "_raw_face_locations", "(", "img", ",", "number_of_times_to_upsample", "=", "1", ",", "model", "=", "\"hog\"", ")", ":", "if", "model", "==", "\"cnn\"", ":", "return", "cnn_face_detector", "(", "img", ",", "number_of_times_to_upsample", ")", "else", ":", "return", "face_detector", "(", "img", ",", "number_of_times_to_upsample", ")"], "docstring": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations", "docstring_tokens": ["Returns", "an", "array", "of", "bounding", "boxes", "of", "human", "faces", "in", "a", "image"], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L92-L105", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "face_locations", "original_string": "def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    if model == \"cnn\":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]", "language": "python", "code": "def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    if model == \"cnn\":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]", "code_tokens": ["def", "face_locations", "(", "img", ",", "number_of_times_to_upsample", "=", "1", ",", "model", "=", "\"hog\"", ")", ":", "if", "model", "==", "\"cnn\"", ":", "return", "[", "_trim_css_to_bounds", "(", "_rect_to_css", "(", "face", ".", "rect", ")", ",", "img", ".", "shape", ")", "for", "face", "in", "_raw_face_locations", "(", "img", ",", "number_of_times_to_upsample", ",", "\"cnn\"", ")", "]", "else", ":", "return", "[", "_trim_css_to_bounds", "(", "_rect_to_css", "(", "face", ")", ",", "img", ".", "shape", ")", "for", "face", "in", "_raw_face_locations", "(", "img", ",", "number_of_times_to_upsample", ",", "model", ")", "]"], "docstring": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order", "docstring_tokens": ["Returns", "an", "array", "of", "bounding", "boxes", "of", "human", "faces", "in", "a", "image"], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L108-L121", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "batch_face_locations", "original_string": "def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    def convert_cnn_detections_to_css(detections):\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))", "language": "python", "code": "def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    def convert_cnn_detections_to_css(detections):\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))", "code_tokens": ["def", "batch_face_locations", "(", "images", ",", "number_of_times_to_upsample", "=", "1", ",", "batch_size", "=", "128", ")", ":", "def", "convert_cnn_detections_to_css", "(", "detections", ")", ":", "return", "[", "_trim_css_to_bounds", "(", "_rect_to_css", "(", "face", ".", "rect", ")", ",", "images", "[", "0", "]", ".", "shape", ")", "for", "face", "in", "detections", "]", "raw_detections_batched", "=", "_raw_face_locations_batched", "(", "images", ",", "number_of_times_to_upsample", ",", "batch_size", ")", "return", "list", "(", "map", "(", "convert_cnn_detections_to_css", ",", "raw_detections_batched", ")", ")"], "docstring": "Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order", "docstring_tokens": ["Returns", "an", "2d", "array", "of", "bounding", "boxes", "of", "human", "faces", "in", "a", "image", "using", "the", "cnn", "face", "detector", "If", "you", "are", "using", "a", "GPU", "this", "can", "give", "you", "much", "faster", "results", "since", "the", "GPU", "can", "process", "batches", "of", "images", "at", "once", ".", "If", "you", "aren", "t", "using", "a", "GPU", "you", "don", "t", "need", "this", "function", "."], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L135-L151", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "face_landmarks", "original_string": "def face_landmarks(face_image, face_locations=None, model=\"large\"):\n    \"\"\"\n    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)\n    \"\"\"\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n    if model == 'large':\n        return [{\n            \"chin\": points[0:17],\n            \"left_eyebrow\": points[17:22],\n            \"right_eyebrow\": points[22:27],\n            \"nose_bridge\": points[27:31],\n            \"nose_tip\": points[31:36],\n            \"left_eye\": points[36:42],\n            \"right_eye\": points[42:48],\n            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n        } for points in landmarks_as_tuples]\n    elif model == 'small':\n        return [{\n            \"nose_tip\": [points[4]],\n            \"left_eye\": points[2:4],\n            \"right_eye\": points[0:2],\n        } for points in landmarks_as_tuples]\n    else:\n        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")", "language": "python", "code": "def face_landmarks(face_image, face_locations=None, model=\"large\"):\n    \"\"\"\n    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)\n    \"\"\"\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n    if model == 'large':\n        return [{\n            \"chin\": points[0:17],\n            \"left_eyebrow\": points[17:22],\n            \"right_eyebrow\": points[22:27],\n            \"nose_bridge\": points[27:31],\n            \"nose_tip\": points[31:36],\n            \"left_eye\": points[36:42],\n            \"right_eye\": points[42:48],\n            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n        } for points in landmarks_as_tuples]\n    elif model == 'small':\n        return [{\n            \"nose_tip\": [points[4]],\n            \"left_eye\": points[2:4],\n            \"right_eye\": points[0:2],\n        } for points in landmarks_as_tuples]\n    else:\n        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")", "code_tokens": ["def", "face_landmarks", "(", "face_image", ",", "face_locations", "=", "None", ",", "model", "=", "\"large\"", ")", ":", "landmarks", "=", "_raw_face_landmarks", "(", "face_image", ",", "face_locations", ",", "model", ")", "landmarks_as_tuples", "=", "[", "[", "(", "p", ".", "x", ",", "p", ".", "y", ")", "for", "p", "in", "landmark", ".", "parts", "(", ")", "]", "for", "landmark", "in", "landmarks", "]", "# For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png", "if", "model", "==", "'large'", ":", "return", "[", "{", "\"chin\"", ":", "points", "[", "0", ":", "17", "]", ",", "\"left_eyebrow\"", ":", "points", "[", "17", ":", "22", "]", ",", "\"right_eyebrow\"", ":", "points", "[", "22", ":", "27", "]", ",", "\"nose_bridge\"", ":", "points", "[", "27", ":", "31", "]", ",", "\"nose_tip\"", ":", "points", "[", "31", ":", "36", "]", ",", "\"left_eye\"", ":", "points", "[", "36", ":", "42", "]", ",", "\"right_eye\"", ":", "points", "[", "42", ":", "48", "]", ",", "\"top_lip\"", ":", "points", "[", "48", ":", "55", "]", "+", "[", "points", "[", "64", "]", "]", "+", "[", "points", "[", "63", "]", "]", "+", "[", "points", "[", "62", "]", "]", "+", "[", "points", "[", "61", "]", "]", "+", "[", "points", "[", "60", "]", "]", ",", "\"bottom_lip\"", ":", "points", "[", "54", ":", "60", "]", "+", "[", "points", "[", "48", "]", "]", "+", "[", "points", "[", "60", "]", "]", "+", "[", "points", "[", "67", "]", "]", "+", "[", "points", "[", "66", "]", "]", "+", "[", "points", "[", "65", "]", "]", "+", "[", "points", "[", "64", "]", "]", "}", "for", "points", "in", "landmarks_as_tuples", "]", "elif", "model", "==", "'small'", ":", "return", "[", "{", "\"nose_tip\"", ":", "[", "points", "[", "4", "]", "]", ",", "\"left_eye\"", ":", "points", "[", "2", ":", "4", "]", ",", "\"right_eye\"", ":", "points", "[", "0", ":", "2", "]", ",", "}", "for", "points", "in", "landmarks_as_tuples", "]", "else", ":", "raise", "ValueError", "(", "\"Invalid landmarks model type. Supported models are ['small', 'large'].\"", ")"], "docstring": "Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)", "docstring_tokens": ["Given", "an", "image", "returns", "a", "dict", "of", "face", "feature", "locations", "(", "eyes", "nose", "etc", ")", "for", "each", "face", "in", "the", "image"], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L168-L200", "partition": "train"}
,{"repo": "ageitgey/face_recognition", "path": "face_recognition/api.py", "func_name": "face_encodings", "original_string": "def face_encodings(face_image, known_face_locations=None, num_jitters=1):\n    \"\"\"\n    Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :return: A list of 128-dimensional face encodings (one for each face in the image)\n    \"\"\"\n    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model=\"small\")\n    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]", "language": "python", "code": "def face_encodings(face_image, known_face_locations=None, num_jitters=1):\n    \"\"\"\n    Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :return: A list of 128-dimensional face encodings (one for each face in the image)\n    \"\"\"\n    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model=\"small\")\n    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]", "code_tokens": ["def", "face_encodings", "(", "face_image", ",", "known_face_locations", "=", "None", ",", "num_jitters", "=", "1", ")", ":", "raw_landmarks", "=", "_raw_face_landmarks", "(", "face_image", ",", "known_face_locations", ",", "model", "=", "\"small\"", ")", "return", "[", "np", ".", "array", "(", "face_encoder", ".", "compute_face_descriptor", "(", "face_image", ",", "raw_landmark_set", ",", "num_jitters", ")", ")", "for", "raw_landmark_set", "in", "raw_landmarks", "]"], "docstring": "Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :return: A list of 128-dimensional face encodings (one for each face in the image)", "docstring_tokens": ["Given", "an", "image", "return", "the", "128", "-", "dimension", "face", "encoding", "for", "each", "face", "in", "the", "image", "."], "sha": "c96b010c02f15e8eeb0f71308c641179ac1f19bb", "url": "https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L203-L213", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/sql/types.py", "func_name": "_parse_datatype_string", "original_string": "def _parse_datatype_string(s):\n    \"\"\"\n    Parses the given data type string to a :class:`DataType`. The data type string format equals\n    to :class:`DataType.simpleString`, except that top level struct type can omit\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\n    string and case-insensitive strings.\n\n    >>> _parse_datatype_string(\"int \")\n    IntegerType\n    >>> _parse_datatype_string(\"INT \")\n    IntegerType\n    >>> _parse_datatype_string(\"a: byte, b: decimal(  16 , 8   ) \")\n    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))\n    >>> _parse_datatype_string(\"a DOUBLE, b STRING\")\n    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))\n    >>> _parse_datatype_string(\"a: array< short>\")\n    StructType(List(StructField(a,ArrayType(ShortType,true),true)))\n    >>> _parse_datatype_string(\" map<string , string > \")\n    MapType(StringType,StringType,true)\n\n    >>> # Error cases\n    >>> _parse_datatype_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    \"\"\"\n    sc = SparkContext._active_spark_context\n\n    def from_ddl_schema(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n\n    def from_ddl_datatype(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.parseDataType(type_str).json())\n\n    try:\n        # DDL format, \"fieldname datatype, fieldname datatype\".\n        return from_ddl_schema(s)\n    except Exception as e:\n        try:\n            # For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\n            return from_ddl_datatype(s)\n        except:\n            try:\n                # For backwards compatibility, \"fieldname: datatype, fieldname: datatype\" case.\n                return from_ddl_datatype(\"struct<%s>\" % s.strip())\n            except:\n                raise e", "language": "python", "code": "def _parse_datatype_string(s):\n    \"\"\"\n    Parses the given data type string to a :class:`DataType`. The data type string format equals\n    to :class:`DataType.simpleString`, except that top level struct type can omit\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\n    string and case-insensitive strings.\n\n    >>> _parse_datatype_string(\"int \")\n    IntegerType\n    >>> _parse_datatype_string(\"INT \")\n    IntegerType\n    >>> _parse_datatype_string(\"a: byte, b: decimal(  16 , 8   ) \")\n    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))\n    >>> _parse_datatype_string(\"a DOUBLE, b STRING\")\n    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))\n    >>> _parse_datatype_string(\"a: array< short>\")\n    StructType(List(StructField(a,ArrayType(ShortType,true),true)))\n    >>> _parse_datatype_string(\" map<string , string > \")\n    MapType(StringType,StringType,true)\n\n    >>> # Error cases\n    >>> _parse_datatype_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    \"\"\"\n    sc = SparkContext._active_spark_context\n\n    def from_ddl_schema(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n\n    def from_ddl_datatype(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.parseDataType(type_str).json())\n\n    try:\n        # DDL format, \"fieldname datatype, fieldname datatype\".\n        return from_ddl_schema(s)\n    except Exception as e:\n        try:\n            # For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\n            return from_ddl_datatype(s)\n        except:\n            try:\n                # For backwards compatibility, \"fieldname: datatype, fieldname: datatype\" case.\n                return from_ddl_datatype(\"struct<%s>\" % s.strip())\n            except:\n                raise e", "code_tokens": ["def", "_parse_datatype_string", "(", "s", ")", ":", "sc", "=", "SparkContext", ".", "_active_spark_context", "def", "from_ddl_schema", "(", "type_str", ")", ":", "return", "_parse_datatype_json_string", "(", "sc", ".", "_jvm", ".", "org", ".", "apache", ".", "spark", ".", "sql", ".", "types", ".", "StructType", ".", "fromDDL", "(", "type_str", ")", ".", "json", "(", ")", ")", "def", "from_ddl_datatype", "(", "type_str", ")", ":", "return", "_parse_datatype_json_string", "(", "sc", ".", "_jvm", ".", "org", ".", "apache", ".", "spark", ".", "sql", ".", "api", ".", "python", ".", "PythonSQLUtils", ".", "parseDataType", "(", "type_str", ")", ".", "json", "(", ")", ")", "try", ":", "# DDL format, \"fieldname datatype, fieldname datatype\".", "return", "from_ddl_schema", "(", "s", ")", "except", "Exception", "as", "e", ":", "try", ":", "# For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.", "return", "from_ddl_datatype", "(", "s", ")", "except", ":", "try", ":", "# For backwards compatibility, \"fieldname: datatype, fieldname: datatype\" case.", "return", "from_ddl_datatype", "(", "\"struct<%s>\"", "%", "s", ".", "strip", "(", ")", ")", "except", ":", "raise", "e"], "docstring": "Parses the given data type string to a :class:`DataType`. The data type string format equals\n    to :class:`DataType.simpleString`, except that top level struct type can omit\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\n    string and case-insensitive strings.\n\n    >>> _parse_datatype_string(\"int \")\n    IntegerType\n    >>> _parse_datatype_string(\"INT \")\n    IntegerType\n    >>> _parse_datatype_string(\"a: byte, b: decimal(  16 , 8   ) \")\n    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))\n    >>> _parse_datatype_string(\"a DOUBLE, b STRING\")\n    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))\n    >>> _parse_datatype_string(\"a: array< short>\")\n    StructType(List(StructField(a,ArrayType(ShortType,true),true)))\n    >>> _parse_datatype_string(\" map<string , string > \")\n    MapType(StringType,StringType,true)\n\n    >>> # Error cases\n    >>> _parse_datatype_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...", "docstring_tokens": ["Parses", "the", "given", "data", "type", "string", "to", "a", ":", "class", ":", "DataType", ".", "The", "data", "type", "string", "format", "equals", "to", ":", "class", ":", "DataType", ".", "simpleString", "except", "that", "top", "level", "struct", "type", "can", "omit", "the", "struct<", ">", "and", "atomic", "types", "use", "typeName", "()", "as", "their", "format", "e", ".", "g", ".", "use", "byte", "instead", "of", "tinyint", "for", ":", "class", ":", "ByteType", ".", "We", "can", "also", "use", "int", "as", "a", "short", "name", "for", ":", "class", ":", "IntegerType", ".", "Since", "Spark", "2", ".", "3", "this", "also", "supports", "a", "schema", "in", "a", "DDL", "-", "formatted", "string", "and", "case", "-", "insensitive", "strings", "."], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L758-L820", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/sql/types.py", "func_name": "_int_size_to_type", "original_string": "def _int_size_to_type(size):\n    \"\"\"\n    Return the Catalyst datatype from the size of integers.\n    \"\"\"\n    if size <= 8:\n        return ByteType\n    if size <= 16:\n        return ShortType\n    if size <= 32:\n        return IntegerType\n    if size <= 64:\n        return LongType", "language": "python", "code": "def _int_size_to_type(size):\n    \"\"\"\n    Return the Catalyst datatype from the size of integers.\n    \"\"\"\n    if size <= 8:\n        return ByteType\n    if size <= 16:\n        return ShortType\n    if size <= 32:\n        return IntegerType\n    if size <= 64:\n        return LongType", "code_tokens": ["def", "_int_size_to_type", "(", "size", ")", ":", "if", "size", "<=", "8", ":", "return", "ByteType", "if", "size", "<=", "16", ":", "return", "ShortType", "if", "size", "<=", "32", ":", "return", "IntegerType", "if", "size", "<=", "64", ":", "return", "LongType"], "docstring": "Return the Catalyst datatype from the size of integers.", "docstring_tokens": ["Return", "the", "Catalyst", "datatype", "from", "the", "size", "of", "integers", "."], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L944-L955", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/sql/types.py", "func_name": "_infer_type", "original_string": "def _infer_type(obj):\n    \"\"\"Infer the DataType from obj\n    \"\"\"\n    if obj is None:\n        return NullType()\n\n    if hasattr(obj, '__UDT__'):\n        return obj.__UDT__\n\n    dataType = _type_mappings.get(type(obj))\n    if dataType is DecimalType:\n        # the precision and scale of `obj` may be different from row to row.\n        return DecimalType(38, 18)\n    elif dataType is not None:\n        return dataType()\n\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            if key is not None and value is not None:\n                return MapType(_infer_type(key), _infer_type(value), True)\n        return MapType(NullType(), NullType(), True)\n    elif isinstance(obj, list):\n        for v in obj:\n            if v is not None:\n                return ArrayType(_infer_type(obj[0]), True)\n        return ArrayType(NullType(), True)\n    elif isinstance(obj, array):\n        if obj.typecode in _array_type_mappings:\n            return ArrayType(_array_type_mappings[obj.typecode](), False)\n        else:\n            raise TypeError(\"not supported type: array(%s)\" % obj.typecode)\n    else:\n        try:\n            return _infer_schema(obj)\n        except TypeError:\n            raise TypeError(\"not supported type: %s\" % type(obj))", "language": "python", "code": "def _infer_type(obj):\n    \"\"\"Infer the DataType from obj\n    \"\"\"\n    if obj is None:\n        return NullType()\n\n    if hasattr(obj, '__UDT__'):\n        return obj.__UDT__\n\n    dataType = _type_mappings.get(type(obj))\n    if dataType is DecimalType:\n        # the precision and scale of `obj` may be different from row to row.\n        return DecimalType(38, 18)\n    elif dataType is not None:\n        return dataType()\n\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            if key is not None and value is not None:\n                return MapType(_infer_type(key), _infer_type(value), True)\n        return MapType(NullType(), NullType(), True)\n    elif isinstance(obj, list):\n        for v in obj:\n            if v is not None:\n                return ArrayType(_infer_type(obj[0]), True)\n        return ArrayType(NullType(), True)\n    elif isinstance(obj, array):\n        if obj.typecode in _array_type_mappings:\n            return ArrayType(_array_type_mappings[obj.typecode](), False)\n        else:\n            raise TypeError(\"not supported type: array(%s)\" % obj.typecode)\n    else:\n        try:\n            return _infer_schema(obj)\n        except TypeError:\n            raise TypeError(\"not supported type: %s\" % type(obj))", "code_tokens": ["def", "_infer_type", "(", "obj", ")", ":", "if", "obj", "is", "None", ":", "return", "NullType", "(", ")", "if", "hasattr", "(", "obj", ",", "'__UDT__'", ")", ":", "return", "obj", ".", "__UDT__", "dataType", "=", "_type_mappings", ".", "get", "(", "type", "(", "obj", ")", ")", "if", "dataType", "is", "DecimalType", ":", "# the precision and scale of `obj` may be different from row to row.", "return", "DecimalType", "(", "38", ",", "18", ")", "elif", "dataType", "is", "not", "None", ":", "return", "dataType", "(", ")", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "for", "key", ",", "value", "in", "obj", ".", "items", "(", ")", ":", "if", "key", "is", "not", "None", "and", "value", "is", "not", "None", ":", "return", "MapType", "(", "_infer_type", "(", "key", ")", ",", "_infer_type", "(", "value", ")", ",", "True", ")", "return", "MapType", "(", "NullType", "(", ")", ",", "NullType", "(", ")", ",", "True", ")", "elif", "isinstance", "(", "obj", ",", "list", ")", ":", "for", "v", "in", "obj", ":", "if", "v", "is", "not", "None", ":", "return", "ArrayType", "(", "_infer_type", "(", "obj", "[", "0", "]", ")", ",", "True", ")", "return", "ArrayType", "(", "NullType", "(", ")", ",", "True", ")", "elif", "isinstance", "(", "obj", ",", "array", ")", ":", "if", "obj", ".", "typecode", "in", "_array_type_mappings", ":", "return", "ArrayType", "(", "_array_type_mappings", "[", "obj", ".", "typecode", "]", "(", ")", ",", "False", ")", "else", ":", "raise", "TypeError", "(", "\"not supported type: array(%s)\"", "%", "obj", ".", "typecode", ")", "else", ":", "try", ":", "return", "_infer_schema", "(", "obj", ")", "except", "TypeError", ":", "raise", "TypeError", "(", "\"not supported type: %s\"", "%", "type", "(", "obj", ")", ")"], "docstring": "Infer the DataType from obj", "docstring_tokens": ["Infer", "the", "DataType", "from", "obj"], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1003-L1038", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/sql/types.py", "func_name": "_infer_schema", "original_string": "def _infer_schema(row, names=None):\n    \"\"\"Infer the schema from dict/namedtuple/object\"\"\"\n    if isinstance(row, dict):\n        items = sorted(row.items())\n\n    elif isinstance(row, (tuple, list)):\n        if hasattr(row, \"__fields__\"):  # Row\n            items = zip(row.__fields__, tuple(row))\n        elif hasattr(row, \"_fields\"):  # namedtuple\n            items = zip(row._fields, tuple(row))\n        else:\n            if names is None:\n                names = ['_%d' % i for i in range(1, len(row) + 1)]\n            elif len(names) < len(row):\n                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))\n            items = zip(names, row)\n\n    elif hasattr(row, \"__dict__\"):  # object\n        items = sorted(row.__dict__.items())\n\n    else:\n        raise TypeError(\"Can not infer schema for type: %s\" % type(row))\n\n    fields = [StructField(k, _infer_type(v), True) for k, v in items]\n    return StructType(fields)", "language": "python", "code": "def _infer_schema(row, names=None):\n    \"\"\"Infer the schema from dict/namedtuple/object\"\"\"\n    if isinstance(row, dict):\n        items = sorted(row.items())\n\n    elif isinstance(row, (tuple, list)):\n        if hasattr(row, \"__fields__\"):  # Row\n            items = zip(row.__fields__, tuple(row))\n        elif hasattr(row, \"_fields\"):  # namedtuple\n            items = zip(row._fields, tuple(row))\n        else:\n            if names is None:\n                names = ['_%d' % i for i in range(1, len(row) + 1)]\n            elif len(names) < len(row):\n                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))\n            items = zip(names, row)\n\n    elif hasattr(row, \"__dict__\"):  # object\n        items = sorted(row.__dict__.items())\n\n    else:\n        raise TypeError(\"Can not infer schema for type: %s\" % type(row))\n\n    fields = [StructField(k, _infer_type(v), True) for k, v in items]\n    return StructType(fields)", "code_tokens": ["def", "_infer_schema", "(", "row", ",", "names", "=", "None", ")", ":", "if", "isinstance", "(", "row", ",", "dict", ")", ":", "items", "=", "sorted", "(", "row", ".", "items", "(", ")", ")", "elif", "isinstance", "(", "row", ",", "(", "tuple", ",", "list", ")", ")", ":", "if", "hasattr", "(", "row", ",", "\"__fields__\"", ")", ":", "# Row", "items", "=", "zip", "(", "row", ".", "__fields__", ",", "tuple", "(", "row", ")", ")", "elif", "hasattr", "(", "row", ",", "\"_fields\"", ")", ":", "# namedtuple", "items", "=", "zip", "(", "row", ".", "_fields", ",", "tuple", "(", "row", ")", ")", "else", ":", "if", "names", "is", "None", ":", "names", "=", "[", "'_%d'", "%", "i", "for", "i", "in", "range", "(", "1", ",", "len", "(", "row", ")", "+", "1", ")", "]", "elif", "len", "(", "names", ")", "<", "len", "(", "row", ")", ":", "names", ".", "extend", "(", "'_%d'", "%", "i", "for", "i", "in", "range", "(", "len", "(", "names", ")", "+", "1", ",", "len", "(", "row", ")", "+", "1", ")", ")", "items", "=", "zip", "(", "names", ",", "row", ")", "elif", "hasattr", "(", "row", ",", "\"__dict__\"", ")", ":", "# object", "items", "=", "sorted", "(", "row", ".", "__dict__", ".", "items", "(", ")", ")", "else", ":", "raise", "TypeError", "(", "\"Can not infer schema for type: %s\"", "%", "type", "(", "row", ")", ")", "fields", "=", "[", "StructField", "(", "k", ",", "_infer_type", "(", "v", ")", ",", "True", ")", "for", "k", ",", "v", "in", "items", "]", "return", "StructType", "(", "fields", ")"], "docstring": "Infer the schema from dict/namedtuple/object", "docstring_tokens": ["Infer", "the", "schema", "from", "dict", "/", "namedtuple", "/", "object"], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1041-L1065", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/sql/types.py", "func_name": "_has_nulltype", "original_string": "def _has_nulltype(dt):\n    \"\"\" Return whether there is NullType in `dt` or not \"\"\"\n    if isinstance(dt, StructType):\n        return any(_has_nulltype(f.dataType) for f in dt.fields)\n    elif isinstance(dt, ArrayType):\n        return _has_nulltype((dt.elementType))\n    elif isinstance(dt, MapType):\n        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)\n    else:\n        return isinstance(dt, NullType)", "language": "python", "code": "def _has_nulltype(dt):\n    \"\"\" Return whether there is NullType in `dt` or not \"\"\"\n    if isinstance(dt, StructType):\n        return any(_has_nulltype(f.dataType) for f in dt.fields)\n    elif isinstance(dt, ArrayType):\n        return _has_nulltype((dt.elementType))\n    elif isinstance(dt, MapType):\n        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)\n    else:\n        return isinstance(dt, NullType)", "code_tokens": ["def", "_has_nulltype", "(", "dt", ")", ":", "if", "isinstance", "(", "dt", ",", "StructType", ")", ":", "return", "any", "(", "_has_nulltype", "(", "f", ".", "dataType", ")", "for", "f", "in", "dt", ".", "fields", ")", "elif", "isinstance", "(", "dt", ",", "ArrayType", ")", ":", "return", "_has_nulltype", "(", "(", "dt", ".", "elementType", ")", ")", "elif", "isinstance", "(", "dt", ",", "MapType", ")", ":", "return", "_has_nulltype", "(", "dt", ".", "keyType", ")", "or", "_has_nulltype", "(", "dt", ".", "valueType", ")", "else", ":", "return", "isinstance", "(", "dt", ",", "NullType", ")"], "docstring": "Return whether there is NullType in `dt` or not", "docstring_tokens": ["Return", "whether", "there", "is", "NullType", "in", "dt", "or", "not"], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1068-L1077", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/sql/types.py", "func_name": "_create_converter", "original_string": "def _create_converter(dataType):\n    \"\"\"Create a converter to drop the names of fields in obj \"\"\"\n    if not _need_converter(dataType):\n        return lambda x: x\n\n    if isinstance(dataType, ArrayType):\n        conv = _create_converter(dataType.elementType)\n        return lambda row: [conv(v) for v in row]\n\n    elif isinstance(dataType, MapType):\n        kconv = _create_converter(dataType.keyType)\n        vconv = _create_converter(dataType.valueType)\n        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())\n\n    elif isinstance(dataType, NullType):\n        return lambda x: None\n\n    elif not isinstance(dataType, StructType):\n        return lambda x: x\n\n    # dataType must be StructType\n    names = [f.name for f in dataType.fields]\n    converters = [_create_converter(f.dataType) for f in dataType.fields]\n    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)\n\n    def convert_struct(obj):\n        if obj is None:\n            return\n\n        if isinstance(obj, (tuple, list)):\n            if convert_fields:\n                return tuple(conv(v) for v, conv in zip(obj, converters))\n            else:\n                return tuple(obj)\n\n        if isinstance(obj, dict):\n            d = obj\n        elif hasattr(obj, \"__dict__\"):  # object\n            d = obj.__dict__\n        else:\n            raise TypeError(\"Unexpected obj type: %s\" % type(obj))\n\n        if convert_fields:\n            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])\n        else:\n            return tuple([d.get(name) for name in names])\n\n    return convert_struct", "language": "python", "code": "def _create_converter(dataType):\n    \"\"\"Create a converter to drop the names of fields in obj \"\"\"\n    if not _need_converter(dataType):\n        return lambda x: x\n\n    if isinstance(dataType, ArrayType):\n        conv = _create_converter(dataType.elementType)\n        return lambda row: [conv(v) for v in row]\n\n    elif isinstance(dataType, MapType):\n        kconv = _create_converter(dataType.keyType)\n        vconv = _create_converter(dataType.valueType)\n        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())\n\n    elif isinstance(dataType, NullType):\n        return lambda x: None\n\n    elif not isinstance(dataType, StructType):\n        return lambda x: x\n\n    # dataType must be StructType\n    names = [f.name for f in dataType.fields]\n    converters = [_create_converter(f.dataType) for f in dataType.fields]\n    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)\n\n    def convert_struct(obj):\n        if obj is None:\n            return\n\n        if isinstance(obj, (tuple, list)):\n            if convert_fields:\n                return tuple(conv(v) for v, conv in zip(obj, converters))\n            else:\n                return tuple(obj)\n\n        if isinstance(obj, dict):\n            d = obj\n        elif hasattr(obj, \"__dict__\"):  # object\n            d = obj.__dict__\n        else:\n            raise TypeError(\"Unexpected obj type: %s\" % type(obj))\n\n        if convert_fields:\n            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])\n        else:\n            return tuple([d.get(name) for name in names])\n\n    return convert_struct", "code_tokens": ["def", "_create_converter", "(", "dataType", ")", ":", "if", "not", "_need_converter", "(", "dataType", ")", ":", "return", "lambda", "x", ":", "x", "if", "isinstance", "(", "dataType", ",", "ArrayType", ")", ":", "conv", "=", "_create_converter", "(", "dataType", ".", "elementType", ")", "return", "lambda", "row", ":", "[", "conv", "(", "v", ")", "for", "v", "in", "row", "]", "elif", "isinstance", "(", "dataType", ",", "MapType", ")", ":", "kconv", "=", "_create_converter", "(", "dataType", ".", "keyType", ")", "vconv", "=", "_create_converter", "(", "dataType", ".", "valueType", ")", "return", "lambda", "row", ":", "dict", "(", "(", "kconv", "(", "k", ")", ",", "vconv", "(", "v", ")", ")", "for", "k", ",", "v", "in", "row", ".", "items", "(", ")", ")", "elif", "isinstance", "(", "dataType", ",", "NullType", ")", ":", "return", "lambda", "x", ":", "None", "elif", "not", "isinstance", "(", "dataType", ",", "StructType", ")", ":", "return", "lambda", "x", ":", "x", "# dataType must be StructType", "names", "=", "[", "f", ".", "name", "for", "f", "in", "dataType", ".", "fields", "]", "converters", "=", "[", "_create_converter", "(", "f", ".", "dataType", ")", "for", "f", "in", "dataType", ".", "fields", "]", "convert_fields", "=", "any", "(", "_need_converter", "(", "f", ".", "dataType", ")", "for", "f", "in", "dataType", ".", "fields", ")", "def", "convert_struct", "(", "obj", ")", ":", "if", "obj", "is", "None", ":", "return", "if", "isinstance", "(", "obj", ",", "(", "tuple", ",", "list", ")", ")", ":", "if", "convert_fields", ":", "return", "tuple", "(", "conv", "(", "v", ")", "for", "v", ",", "conv", "in", "zip", "(", "obj", ",", "converters", ")", ")", "else", ":", "return", "tuple", "(", "obj", ")", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "d", "=", "obj", "elif", "hasattr", "(", "obj", ",", "\"__dict__\"", ")", ":", "# object", "d", "=", "obj", ".", "__dict__", "else", ":", "raise", "TypeError", "(", "\"Unexpected obj type: %s\"", "%", "type", "(", "obj", ")", ")", "if", "convert_fields", ":", "return", "tuple", "(", "[", "conv", "(", "d", ".", "get", "(", "name", ")", ")", "for", "name", ",", "conv", "in", "zip", "(", "names", ",", "converters", ")", "]", ")", "else", ":", "return", "tuple", "(", "[", "d", ".", "get", "(", "name", ")", "for", "name", "in", "names", "]", ")", "return", "convert_struct"], "docstring": "Create a converter to drop the names of fields in obj", "docstring_tokens": ["Create", "a", "converter", "to", "drop", "the", "names", "of", "fields", "in", "obj"], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1133-L1180", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/sql/types.py", "func_name": "_make_type_verifier", "original_string": "def _make_type_verifier(dataType, nullable=True, name=None):\n    \"\"\"\n    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\n    not match.\n\n    This verifier also checks the value of obj against datatype and raises a ValueError if it's not\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\n    not checked, so it will become infinity when cast to Java float if it overflows.\n\n    >>> _make_type_verifier(StructType([]))(None)\n    >>> _make_type_verifier(StringType())(\"\")\n    >>> _make_type_verifier(LongType())(0)\n    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))\n    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})\n    >>> _make_type_verifier(StructType([]))(())\n    >>> _make_type_verifier(StructType([]))([])\n    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> # Check if numeric values are within the allowed range.\n    >>> _make_type_verifier(ByteType())(12)\n    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(\n    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> schema = StructType().add(\"a\", IntegerType()).add(\"b\", StringType(), False)\n    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    \"\"\"\n\n    if name is None:\n        new_msg = lambda msg: msg\n        new_name = lambda n: \"field %s\" % n\n    else:\n        new_msg = lambda msg: \"%s: %s\" % (name, msg)\n        new_name = lambda n: \"field %s in %s\" % (n, name)\n\n    def verify_nullability(obj):\n        if obj is None:\n            if nullable:\n                return True\n            else:\n                raise ValueError(new_msg(\"This field is not nullable, but got None\"))\n        else:\n            return False\n\n    _type = type(dataType)\n\n    def assert_acceptable_types(obj):\n        assert _type in _acceptable_types, \\\n            new_msg(\"unknown datatype: %s for object %r\" % (dataType, obj))\n\n    def verify_acceptable_types(obj):\n        # subclass of them can not be fromInternal in JVM\n        if type(obj) not in _acceptable_types[_type]:\n            raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n                                    % (dataType, obj, type(obj))))\n\n    if isinstance(dataType, StringType):\n        # StringType can work with any types\n        verify_value = lambda _: _\n\n    elif isinstance(dataType, UserDefinedType):\n        verifier = _make_type_verifier(dataType.sqlType(), name=name)\n\n        def verify_udf(obj):\n            if not (hasattr(obj, '__UDT__') and obj.__UDT__ == dataType):\n                raise ValueError(new_msg(\"%r is not an instance of type %r\" % (obj, dataType)))\n            verifier(dataType.toInternal(obj))\n\n        verify_value = verify_udf\n\n    elif isinstance(dataType, ByteType):\n        def verify_byte(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -128 or obj > 127:\n                raise ValueError(new_msg(\"object of ByteType out of range, got: %s\" % obj))\n\n        verify_value = verify_byte\n\n    elif isinstance(dataType, ShortType):\n        def verify_short(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -32768 or obj > 32767:\n                raise ValueError(new_msg(\"object of ShortType out of range, got: %s\" % obj))\n\n        verify_value = verify_short\n\n    elif isinstance(dataType, IntegerType):\n        def verify_integer(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -2147483648 or obj > 2147483647:\n                raise ValueError(\n                    new_msg(\"object of IntegerType out of range, got: %s\" % obj))\n\n        verify_value = verify_integer\n\n    elif isinstance(dataType, ArrayType):\n        element_verifier = _make_type_verifier(\n            dataType.elementType, dataType.containsNull, name=\"element in array %s\" % name)\n\n        def verify_array(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            for i in obj:\n                element_verifier(i)\n\n        verify_value = verify_array\n\n    elif isinstance(dataType, MapType):\n        key_verifier = _make_type_verifier(dataType.keyType, False, name=\"key of map %s\" % name)\n        value_verifier = _make_type_verifier(\n            dataType.valueType, dataType.valueContainsNull, name=\"value of map %s\" % name)\n\n        def verify_map(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            for k, v in obj.items():\n                key_verifier(k)\n                value_verifier(v)\n\n        verify_value = verify_map\n\n    elif isinstance(dataType, StructType):\n        verifiers = []\n        for f in dataType.fields:\n            verifier = _make_type_verifier(f.dataType, f.nullable, name=new_name(f.name))\n            verifiers.append((f.name, verifier))\n\n        def verify_struct(obj):\n            assert_acceptable_types(obj)\n\n            if isinstance(obj, dict):\n                for f, verifier in verifiers:\n                    verifier(obj.get(f))\n            elif isinstance(obj, Row) and getattr(obj, \"__from_dict__\", False):\n                # the order in obj could be different than dataType.fields\n                for f, verifier in verifiers:\n                    verifier(obj[f])\n            elif isinstance(obj, (tuple, list)):\n                if len(obj) != len(verifiers):\n                    raise ValueError(\n                        new_msg(\"Length of object (%d) does not match with \"\n                                \"length of fields (%d)\" % (len(obj), len(verifiers))))\n                for v, (_, verifier) in zip(obj, verifiers):\n                    verifier(v)\n            elif hasattr(obj, \"__dict__\"):\n                d = obj.__dict__\n                for f, verifier in verifiers:\n                    verifier(d.get(f))\n            else:\n                raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\n                                        % (obj, type(obj))))\n        verify_value = verify_struct\n\n    else:\n        def verify_default(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n\n        verify_value = verify_default\n\n    def verify(obj):\n        if not verify_nullability(obj):\n            verify_value(obj)\n\n    return verify", "language": "python", "code": "def _make_type_verifier(dataType, nullable=True, name=None):\n    \"\"\"\n    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\n    not match.\n\n    This verifier also checks the value of obj against datatype and raises a ValueError if it's not\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\n    not checked, so it will become infinity when cast to Java float if it overflows.\n\n    >>> _make_type_verifier(StructType([]))(None)\n    >>> _make_type_verifier(StringType())(\"\")\n    >>> _make_type_verifier(LongType())(0)\n    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))\n    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})\n    >>> _make_type_verifier(StructType([]))(())\n    >>> _make_type_verifier(StructType([]))([])\n    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> # Check if numeric values are within the allowed range.\n    >>> _make_type_verifier(ByteType())(12)\n    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(\n    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> schema = StructType().add(\"a\", IntegerType()).add(\"b\", StringType(), False)\n    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    \"\"\"\n\n    if name is None:\n        new_msg = lambda msg: msg\n        new_name = lambda n: \"field %s\" % n\n    else:\n        new_msg = lambda msg: \"%s: %s\" % (name, msg)\n        new_name = lambda n: \"field %s in %s\" % (n, name)\n\n    def verify_nullability(obj):\n        if obj is None:\n            if nullable:\n                return True\n            else:\n                raise ValueError(new_msg(\"This field is not nullable, but got None\"))\n        else:\n            return False\n\n    _type = type(dataType)\n\n    def assert_acceptable_types(obj):\n        assert _type in _acceptable_types, \\\n            new_msg(\"unknown datatype: %s for object %r\" % (dataType, obj))\n\n    def verify_acceptable_types(obj):\n        # subclass of them can not be fromInternal in JVM\n        if type(obj) not in _acceptable_types[_type]:\n            raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n                                    % (dataType, obj, type(obj))))\n\n    if isinstance(dataType, StringType):\n        # StringType can work with any types\n        verify_value = lambda _: _\n\n    elif isinstance(dataType, UserDefinedType):\n        verifier = _make_type_verifier(dataType.sqlType(), name=name)\n\n        def verify_udf(obj):\n            if not (hasattr(obj, '__UDT__') and obj.__UDT__ == dataType):\n                raise ValueError(new_msg(\"%r is not an instance of type %r\" % (obj, dataType)))\n            verifier(dataType.toInternal(obj))\n\n        verify_value = verify_udf\n\n    elif isinstance(dataType, ByteType):\n        def verify_byte(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -128 or obj > 127:\n                raise ValueError(new_msg(\"object of ByteType out of range, got: %s\" % obj))\n\n        verify_value = verify_byte\n\n    elif isinstance(dataType, ShortType):\n        def verify_short(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -32768 or obj > 32767:\n                raise ValueError(new_msg(\"object of ShortType out of range, got: %s\" % obj))\n\n        verify_value = verify_short\n\n    elif isinstance(dataType, IntegerType):\n        def verify_integer(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -2147483648 or obj > 2147483647:\n                raise ValueError(\n                    new_msg(\"object of IntegerType out of range, got: %s\" % obj))\n\n        verify_value = verify_integer\n\n    elif isinstance(dataType, ArrayType):\n        element_verifier = _make_type_verifier(\n            dataType.elementType, dataType.containsNull, name=\"element in array %s\" % name)\n\n        def verify_array(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            for i in obj:\n                element_verifier(i)\n\n        verify_value = verify_array\n\n    elif isinstance(dataType, MapType):\n        key_verifier = _make_type_verifier(dataType.keyType, False, name=\"key of map %s\" % name)\n        value_verifier = _make_type_verifier(\n            dataType.valueType, dataType.valueContainsNull, name=\"value of map %s\" % name)\n\n        def verify_map(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            for k, v in obj.items():\n                key_verifier(k)\n                value_verifier(v)\n\n        verify_value = verify_map\n\n    elif isinstance(dataType, StructType):\n        verifiers = []\n        for f in dataType.fields:\n            verifier = _make_type_verifier(f.dataType, f.nullable, name=new_name(f.name))\n            verifiers.append((f.name, verifier))\n\n        def verify_struct(obj):\n            assert_acceptable_types(obj)\n\n            if isinstance(obj, dict):\n                for f, verifier in verifiers:\n                    verifier(obj.get(f))\n            elif isinstance(obj, Row) and getattr(obj, \"__from_dict__\", False):\n                # the order in obj could be different than dataType.fields\n                for f, verifier in verifiers:\n                    verifier(obj[f])\n            elif isinstance(obj, (tuple, list)):\n                if len(obj) != len(verifiers):\n                    raise ValueError(\n                        new_msg(\"Length of object (%d) does not match with \"\n                                \"length of fields (%d)\" % (len(obj), len(verifiers))))\n                for v, (_, verifier) in zip(obj, verifiers):\n                    verifier(v)\n            elif hasattr(obj, \"__dict__\"):\n                d = obj.__dict__\n                for f, verifier in verifiers:\n                    verifier(d.get(f))\n            else:\n                raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\n                                        % (obj, type(obj))))\n        verify_value = verify_struct\n\n    else:\n        def verify_default(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n\n        verify_value = verify_default\n\n    def verify(obj):\n        if not verify_nullability(obj):\n            verify_value(obj)\n\n    return verify", "code_tokens": ["def", "_make_type_verifier", "(", "dataType", ",", "nullable", "=", "True", ",", "name", "=", "None", ")", ":", "if", "name", "is", "None", ":", "new_msg", "=", "lambda", "msg", ":", "msg", "new_name", "=", "lambda", "n", ":", "\"field %s\"", "%", "n", "else", ":", "new_msg", "=", "lambda", "msg", ":", "\"%s: %s\"", "%", "(", "name", ",", "msg", ")", "new_name", "=", "lambda", "n", ":", "\"field %s in %s\"", "%", "(", "n", ",", "name", ")", "def", "verify_nullability", "(", "obj", ")", ":", "if", "obj", "is", "None", ":", "if", "nullable", ":", "return", "True", "else", ":", "raise", "ValueError", "(", "new_msg", "(", "\"This field is not nullable, but got None\"", ")", ")", "else", ":", "return", "False", "_type", "=", "type", "(", "dataType", ")", "def", "assert_acceptable_types", "(", "obj", ")", ":", "assert", "_type", "in", "_acceptable_types", ",", "new_msg", "(", "\"unknown datatype: %s for object %r\"", "%", "(", "dataType", ",", "obj", ")", ")", "def", "verify_acceptable_types", "(", "obj", ")", ":", "# subclass of them can not be fromInternal in JVM", "if", "type", "(", "obj", ")", "not", "in", "_acceptable_types", "[", "_type", "]", ":", "raise", "TypeError", "(", "new_msg", "(", "\"%s can not accept object %r in type %s\"", "%", "(", "dataType", ",", "obj", ",", "type", "(", "obj", ")", ")", ")", ")", "if", "isinstance", "(", "dataType", ",", "StringType", ")", ":", "# StringType can work with any types", "verify_value", "=", "lambda", "_", ":", "_", "elif", "isinstance", "(", "dataType", ",", "UserDefinedType", ")", ":", "verifier", "=", "_make_type_verifier", "(", "dataType", ".", "sqlType", "(", ")", ",", "name", "=", "name", ")", "def", "verify_udf", "(", "obj", ")", ":", "if", "not", "(", "hasattr", "(", "obj", ",", "'__UDT__'", ")", "and", "obj", ".", "__UDT__", "==", "dataType", ")", ":", "raise", "ValueError", "(", "new_msg", "(", "\"%r is not an instance of type %r\"", "%", "(", "obj", ",", "dataType", ")", ")", ")", "verifier", "(", "dataType", ".", "toInternal", "(", "obj", ")", ")", "verify_value", "=", "verify_udf", "elif", "isinstance", "(", "dataType", ",", "ByteType", ")", ":", "def", "verify_byte", "(", "obj", ")", ":", "assert_acceptable_types", "(", "obj", ")", "verify_acceptable_types", "(", "obj", ")", "if", "obj", "<", "-", "128", "or", "obj", ">", "127", ":", "raise", "ValueError", "(", "new_msg", "(", "\"object of ByteType out of range, got: %s\"", "%", "obj", ")", ")", "verify_value", "=", "verify_byte", "elif", "isinstance", "(", "dataType", ",", "ShortType", ")", ":", "def", "verify_short", "(", "obj", ")", ":", "assert_acceptable_types", "(", "obj", ")", "verify_acceptable_types", "(", "obj", ")", "if", "obj", "<", "-", "32768", "or", "obj", ">", "32767", ":", "raise", "ValueError", "(", "new_msg", "(", "\"object of ShortType out of range, got: %s\"", "%", "obj", ")", ")", "verify_value", "=", "verify_short", "elif", "isinstance", "(", "dataType", ",", "IntegerType", ")", ":", "def", "verify_integer", "(", "obj", ")", ":", "assert_acceptable_types", "(", "obj", ")", "verify_acceptable_types", "(", "obj", ")", "if", "obj", "<", "-", "2147483648", "or", "obj", ">", "2147483647", ":", "raise", "ValueError", "(", "new_msg", "(", "\"object of IntegerType out of range, got: %s\"", "%", "obj", ")", ")", "verify_value", "=", "verify_integer", "elif", "isinstance", "(", "dataType", ",", "ArrayType", ")", ":", "element_verifier", "=", "_make_type_verifier", "(", "dataType", ".", "elementType", ",", "dataType", ".", "containsNull", ",", "name", "=", "\"element in array %s\"", "%", "name", ")", "def", "verify_array", "(", "obj", ")", ":", "assert_acceptable_types", "(", "obj", ")", "verify_acceptable_types", "(", "obj", ")", "for", "i", "in", "obj", ":", "element_verifier", "(", "i", ")", "verify_value", "=", "verify_array", "elif", "isinstance", "(", "dataType", ",", "MapType", ")", ":", "key_verifier", "=", "_make_type_verifier", "(", "dataType", ".", "keyType", ",", "False", ",", "name", "=", "\"key of map %s\"", "%", "name", ")", "value_verifier", "=", "_make_type_verifier", "(", "dataType", ".", "valueType", ",", "dataType", ".", "valueContainsNull", ",", "name", "=", "\"value of map %s\"", "%", "name", ")", "def", "verify_map", "(", "obj", ")", ":", "assert_acceptable_types", "(", "obj", ")", "verify_acceptable_types", "(", "obj", ")", "for", "k", ",", "v", "in", "obj", ".", "items", "(", ")", ":", "key_verifier", "(", "k", ")", "value_verifier", "(", "v", ")", "verify_value", "=", "verify_map", "elif", "isinstance", "(", "dataType", ",", "StructType", ")", ":", "verifiers", "=", "[", "]", "for", "f", "in", "dataType", ".", "fields", ":", "verifier", "=", "_make_type_verifier", "(", "f", ".", "dataType", ",", "f", ".", "nullable", ",", "name", "=", "new_name", "(", "f", ".", "name", ")", ")", "verifiers", ".", "append", "(", "(", "f", ".", "name", ",", "verifier", ")", ")", "def", "verify_struct", "(", "obj", ")", ":", "assert_acceptable_types", "(", "obj", ")", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "for", "f", ",", "verifier", "in", "verifiers", ":", "verifier", "(", "obj", ".", "get", "(", "f", ")", ")", "elif", "isinstance", "(", "obj", ",", "Row", ")", "and", "getattr", "(", "obj", ",", "\"__from_dict__\"", ",", "False", ")", ":", "# the order in obj could be different than dataType.fields", "for", "f", ",", "verifier", "in", "verifiers", ":", "verifier", "(", "obj", "[", "f", "]", ")", "elif", "isinstance", "(", "obj", ",", "(", "tuple", ",", "list", ")", ")", ":", "if", "len", "(", "obj", ")", "!=", "len", "(", "verifiers", ")", ":", "raise", "ValueError", "(", "new_msg", "(", "\"Length of object (%d) does not match with \"", "\"length of fields (%d)\"", "%", "(", "len", "(", "obj", ")", ",", "len", "(", "verifiers", ")", ")", ")", ")", "for", "v", ",", "(", "_", ",", "verifier", ")", "in", "zip", "(", "obj", ",", "verifiers", ")", ":", "verifier", "(", "v", ")", "elif", "hasattr", "(", "obj", ",", "\"__dict__\"", ")", ":", "d", "=", "obj", ".", "__dict__", "for", "f", ",", "verifier", "in", "verifiers", ":", "verifier", "(", "d", ".", "get", "(", "f", ")", ")", "else", ":", "raise", "TypeError", "(", "new_msg", "(", "\"StructType can not accept object %r in type %s\"", "%", "(", "obj", ",", "type", "(", "obj", ")", ")", ")", ")", "verify_value", "=", "verify_struct", "else", ":", "def", "verify_default", "(", "obj", ")", ":", "assert_acceptable_types", "(", "obj", ")", "verify_acceptable_types", "(", "obj", ")", "verify_value", "=", "verify_default", "def", "verify", "(", "obj", ")", ":", "if", "not", "verify_nullability", "(", "obj", ")", ":", "verify_value", "(", "obj", ")", "return", "verify"], "docstring": "Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\n    not match.\n\n    This verifier also checks the value of obj against datatype and raises a ValueError if it's not\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\n    not checked, so it will become infinity when cast to Java float if it overflows.\n\n    >>> _make_type_verifier(StructType([]))(None)\n    >>> _make_type_verifier(StringType())(\"\")\n    >>> _make_type_verifier(LongType())(0)\n    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))\n    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})\n    >>> _make_type_verifier(StructType([]))(())\n    >>> _make_type_verifier(StructType([]))([])\n    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> # Check if numeric values are within the allowed range.\n    >>> _make_type_verifier(ByteType())(12)\n    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(\n    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> schema = StructType().add(\"a\", IntegerType()).add(\"b\", StringType(), False)\n    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...", "docstring_tokens": ["Make", "a", "verifier", "that", "checks", "the", "type", "of", "obj", "against", "dataType", "and", "raises", "a", "TypeError", "if", "they", "do", "not", "match", "."], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1202-L1391", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/heapq3.py", "func_name": "merge", "original_string": "def merge(iterables, key=None, reverse=False):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    If *key* is not None, applies a key function to each element to determine\n    its sort order.\n\n    >>> list(merge(['dog', 'horse'], ['cat', 'fish', 'kangaroo'], key=len))\n    ['dog', 'cat', 'fish', 'horse', 'kangaroo']\n\n    '''\n\n    h = []\n    h_append = h.append\n\n    if reverse:\n        _heapify = _heapify_max\n        _heappop = _heappop_max\n        _heapreplace = _heapreplace_max\n        direction = -1\n    else:\n        _heapify = heapify\n        _heappop = heappop\n        _heapreplace = heapreplace\n        direction = 1\n\n    if key is None:\n        for order, it in enumerate(map(iter, iterables)):\n            try:\n                h_append([next(it), order * direction, it])\n            except StopIteration:\n                pass\n        _heapify(h)\n        while len(h) > 1:\n            try:\n                while True:\n                    value, order, it = s = h[0]\n                    yield value\n                    s[0] = next(it)           # raises StopIteration when exhausted\n                    _heapreplace(h, s)      # restore heap condition\n            except StopIteration:\n                _heappop(h)                 # remove empty iterator\n        if h:\n            # fast case when only a single iterator remains\n            value, order, it = h[0]\n            yield value\n            for value in it:\n                yield value\n        return\n\n    for order, it in enumerate(map(iter, iterables)):\n        try:\n            value = next(it)\n            h_append([key(value), order * direction, value, it])\n        except StopIteration:\n            pass\n    _heapify(h)\n    while len(h) > 1:\n        try:\n            while True:\n                key_value, order, value, it = s = h[0]\n                yield value\n                value = next(it)\n                s[0] = key(value)\n                s[2] = value\n                _heapreplace(h, s)\n        except StopIteration:\n            _heappop(h)\n    if h:\n        key_value, order, value, it = h[0]\n        yield value\n        for value in it:\n            yield value", "language": "python", "code": "def merge(iterables, key=None, reverse=False):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    If *key* is not None, applies a key function to each element to determine\n    its sort order.\n\n    >>> list(merge(['dog', 'horse'], ['cat', 'fish', 'kangaroo'], key=len))\n    ['dog', 'cat', 'fish', 'horse', 'kangaroo']\n\n    '''\n\n    h = []\n    h_append = h.append\n\n    if reverse:\n        _heapify = _heapify_max\n        _heappop = _heappop_max\n        _heapreplace = _heapreplace_max\n        direction = -1\n    else:\n        _heapify = heapify\n        _heappop = heappop\n        _heapreplace = heapreplace\n        direction = 1\n\n    if key is None:\n        for order, it in enumerate(map(iter, iterables)):\n            try:\n                h_append([next(it), order * direction, it])\n            except StopIteration:\n                pass\n        _heapify(h)\n        while len(h) > 1:\n            try:\n                while True:\n                    value, order, it = s = h[0]\n                    yield value\n                    s[0] = next(it)           # raises StopIteration when exhausted\n                    _heapreplace(h, s)      # restore heap condition\n            except StopIteration:\n                _heappop(h)                 # remove empty iterator\n        if h:\n            # fast case when only a single iterator remains\n            value, order, it = h[0]\n            yield value\n            for value in it:\n                yield value\n        return\n\n    for order, it in enumerate(map(iter, iterables)):\n        try:\n            value = next(it)\n            h_append([key(value), order * direction, value, it])\n        except StopIteration:\n            pass\n    _heapify(h)\n    while len(h) > 1:\n        try:\n            while True:\n                key_value, order, value, it = s = h[0]\n                yield value\n                value = next(it)\n                s[0] = key(value)\n                s[2] = value\n                _heapreplace(h, s)\n        except StopIteration:\n            _heappop(h)\n    if h:\n        key_value, order, value, it = h[0]\n        yield value\n        for value in it:\n            yield value", "code_tokens": ["def", "merge", "(", "iterables", ",", "key", "=", "None", ",", "reverse", "=", "False", ")", ":", "h", "=", "[", "]", "h_append", "=", "h", ".", "append", "if", "reverse", ":", "_heapify", "=", "_heapify_max", "_heappop", "=", "_heappop_max", "_heapreplace", "=", "_heapreplace_max", "direction", "=", "-", "1", "else", ":", "_heapify", "=", "heapify", "_heappop", "=", "heappop", "_heapreplace", "=", "heapreplace", "direction", "=", "1", "if", "key", "is", "None", ":", "for", "order", ",", "it", "in", "enumerate", "(", "map", "(", "iter", ",", "iterables", ")", ")", ":", "try", ":", "h_append", "(", "[", "next", "(", "it", ")", ",", "order", "*", "direction", ",", "it", "]", ")", "except", "StopIteration", ":", "pass", "_heapify", "(", "h", ")", "while", "len", "(", "h", ")", ">", "1", ":", "try", ":", "while", "True", ":", "value", ",", "order", ",", "it", "=", "s", "=", "h", "[", "0", "]", "yield", "value", "s", "[", "0", "]", "=", "next", "(", "it", ")", "# raises StopIteration when exhausted", "_heapreplace", "(", "h", ",", "s", ")", "# restore heap condition", "except", "StopIteration", ":", "_heappop", "(", "h", ")", "# remove empty iterator", "if", "h", ":", "# fast case when only a single iterator remains", "value", ",", "order", ",", "it", "=", "h", "[", "0", "]", "yield", "value", "for", "value", "in", "it", ":", "yield", "value", "return", "for", "order", ",", "it", "in", "enumerate", "(", "map", "(", "iter", ",", "iterables", ")", ")", ":", "try", ":", "value", "=", "next", "(", "it", ")", "h_append", "(", "[", "key", "(", "value", ")", ",", "order", "*", "direction", ",", "value", ",", "it", "]", ")", "except", "StopIteration", ":", "pass", "_heapify", "(", "h", ")", "while", "len", "(", "h", ")", ">", "1", ":", "try", ":", "while", "True", ":", "key_value", ",", "order", ",", "value", ",", "it", "=", "s", "=", "h", "[", "0", "]", "yield", "value", "value", "=", "next", "(", "it", ")", "s", "[", "0", "]", "=", "key", "(", "value", ")", "s", "[", "2", "]", "=", "value", "_heapreplace", "(", "h", ",", "s", ")", "except", "StopIteration", ":", "_heappop", "(", "h", ")", "if", "h", ":", "key_value", ",", "order", ",", "value", ",", "it", "=", "h", "[", "0", "]", "yield", "value", "for", "value", "in", "it", ":", "yield", "value"], "docstring": "Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    If *key* is not None, applies a key function to each element to determine\n    its sort order.\n\n    >>> list(merge(['dog', 'horse'], ['cat', 'fish', 'kangaroo'], key=len))\n    ['dog', 'cat', 'fish', 'horse', 'kangaroo']", "docstring_tokens": ["Merge", "multiple", "sorted", "inputs", "into", "a", "single", "sorted", "output", "."], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/heapq3.py#L595-L673", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/heapq3.py", "func_name": "nsmallest", "original_string": "def nsmallest(n, iterable, key=None):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use min()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = min(it, default=sentinel)\n        else:\n            result = min(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        # put the range(n) first so that zip() doesn't\n        # consume one too many elements from the iterator\n        result = [(elem, i) for i, elem in zip(range(n), it)]\n        if not result:\n            return result\n        _heapify_max(result)\n        top = result[0][0]\n        order = n\n        _heapreplace = _heapreplace_max\n        for elem in it:\n            if elem < top:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order += 1\n        result.sort()\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(n), it)]\n    if not result:\n        return result\n    _heapify_max(result)\n    top = result[0][0]\n    order = n\n    _heapreplace = _heapreplace_max\n    for elem in it:\n        k = key(elem)\n        if k < top:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order += 1\n    result.sort()\n    return [r[2] for r in result]", "language": "python", "code": "def nsmallest(n, iterable, key=None):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use min()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = min(it, default=sentinel)\n        else:\n            result = min(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        # put the range(n) first so that zip() doesn't\n        # consume one too many elements from the iterator\n        result = [(elem, i) for i, elem in zip(range(n), it)]\n        if not result:\n            return result\n        _heapify_max(result)\n        top = result[0][0]\n        order = n\n        _heapreplace = _heapreplace_max\n        for elem in it:\n            if elem < top:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order += 1\n        result.sort()\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(n), it)]\n    if not result:\n        return result\n    _heapify_max(result)\n    top = result[0][0]\n    order = n\n    _heapreplace = _heapreplace_max\n    for elem in it:\n        k = key(elem)\n        if k < top:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order += 1\n    result.sort()\n    return [r[2] for r in result]", "code_tokens": ["def", "nsmallest", "(", "n", ",", "iterable", ",", "key", "=", "None", ")", ":", "# Short-cut for n==1 is to use min()", "if", "n", "==", "1", ":", "it", "=", "iter", "(", "iterable", ")", "sentinel", "=", "object", "(", ")", "if", "key", "is", "None", ":", "result", "=", "min", "(", "it", ",", "default", "=", "sentinel", ")", "else", ":", "result", "=", "min", "(", "it", ",", "default", "=", "sentinel", ",", "key", "=", "key", ")", "return", "[", "]", "if", "result", "is", "sentinel", "else", "[", "result", "]", "# When n>=size, it's faster to use sorted()", "try", ":", "size", "=", "len", "(", "iterable", ")", "except", "(", "TypeError", ",", "AttributeError", ")", ":", "pass", "else", ":", "if", "n", ">=", "size", ":", "return", "sorted", "(", "iterable", ",", "key", "=", "key", ")", "[", ":", "n", "]", "# When key is none, use simpler decoration", "if", "key", "is", "None", ":", "it", "=", "iter", "(", "iterable", ")", "# put the range(n) first so that zip() doesn't", "# consume one too many elements from the iterator", "result", "=", "[", "(", "elem", ",", "i", ")", "for", "i", ",", "elem", "in", "zip", "(", "range", "(", "n", ")", ",", "it", ")", "]", "if", "not", "result", ":", "return", "result", "_heapify_max", "(", "result", ")", "top", "=", "result", "[", "0", "]", "[", "0", "]", "order", "=", "n", "_heapreplace", "=", "_heapreplace_max", "for", "elem", "in", "it", ":", "if", "elem", "<", "top", ":", "_heapreplace", "(", "result", ",", "(", "elem", ",", "order", ")", ")", "top", "=", "result", "[", "0", "]", "[", "0", "]", "order", "+=", "1", "result", ".", "sort", "(", ")", "return", "[", "r", "[", "0", "]", "for", "r", "in", "result", "]", "# General case, slowest method", "it", "=", "iter", "(", "iterable", ")", "result", "=", "[", "(", "key", "(", "elem", ")", ",", "i", ",", "elem", ")", "for", "i", ",", "elem", "in", "zip", "(", "range", "(", "n", ")", ",", "it", ")", "]", "if", "not", "result", ":", "return", "result", "_heapify_max", "(", "result", ")", "top", "=", "result", "[", "0", "]", "[", "0", "]", "order", "=", "n", "_heapreplace", "=", "_heapreplace_max", "for", "elem", "in", "it", ":", "k", "=", "key", "(", "elem", ")", "if", "k", "<", "top", ":", "_heapreplace", "(", "result", ",", "(", "k", ",", "order", ",", "elem", ")", ")", "top", "=", "result", "[", "0", "]", "[", "0", "]", "order", "+=", "1", "result", ".", "sort", "(", ")", "return", "[", "r", "[", "2", "]", "for", "r", "in", "result", "]"], "docstring": "Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]", "docstring_tokens": ["Find", "the", "n", "smallest", "elements", "in", "a", "dataset", "."], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/heapq3.py#L742-L803", "partition": "train"}
,{"repo": "apache/spark", "path": "python/pyspark/heapq3.py", "func_name": "nlargest", "original_string": "def nlargest(n, iterable, key=None):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use max()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = max(it, default=sentinel)\n        else:\n            result = max(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        result = [(elem, i) for i, elem in zip(range(0, -n, -1), it)]\n        if not result:\n            return result\n        heapify(result)\n        top = result[0][0]\n        order = -n\n        _heapreplace = heapreplace\n        for elem in it:\n            if top < elem:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order -= 1\n        result.sort(reverse=True)\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]\n    if not result:\n        return result\n    heapify(result)\n    top = result[0][0]\n    order = -n\n    _heapreplace = heapreplace\n    for elem in it:\n        k = key(elem)\n        if top < k:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order -= 1\n    result.sort(reverse=True)\n    return [r[2] for r in result]", "language": "python", "code": "def nlargest(n, iterable, key=None):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use max()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = max(it, default=sentinel)\n        else:\n            result = max(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        result = [(elem, i) for i, elem in zip(range(0, -n, -1), it)]\n        if not result:\n            return result\n        heapify(result)\n        top = result[0][0]\n        order = -n\n        _heapreplace = heapreplace\n        for elem in it:\n            if top < elem:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order -= 1\n        result.sort(reverse=True)\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]\n    if not result:\n        return result\n    heapify(result)\n    top = result[0][0]\n    order = -n\n    _heapreplace = heapreplace\n    for elem in it:\n        k = key(elem)\n        if top < k:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order -= 1\n    result.sort(reverse=True)\n    return [r[2] for r in result]", "code_tokens": ["def", "nlargest", "(", "n", ",", "iterable", ",", "key", "=", "None", ")", ":", "# Short-cut for n==1 is to use max()", "if", "n", "==", "1", ":", "it", "=", "iter", "(", "iterable", ")", "sentinel", "=", "object", "(", ")", "if", "key", "is", "None", ":", "result", "=", "max", "(", "it", ",", "default", "=", "sentinel", ")", "else", ":", "result", "=", "max", "(", "it", ",", "default", "=", "sentinel", ",", "key", "=", "key", ")", "return", "[", "]", "if", "result", "is", "sentinel", "else", "[", "result", "]", "# When n>=size, it's faster to use sorted()", "try", ":", "size", "=", "len", "(", "iterable", ")", "except", "(", "TypeError", ",", "AttributeError", ")", ":", "pass", "else", ":", "if", "n", ">=", "size", ":", "return", "sorted", "(", "iterable", ",", "key", "=", "key", ",", "reverse", "=", "True", ")", "[", ":", "n", "]", "# When key is none, use simpler decoration", "if", "key", "is", "None", ":", "it", "=", "iter", "(", "iterable", ")", "result", "=", "[", "(", "elem", ",", "i", ")", "for", "i", ",", "elem", "in", "zip", "(", "range", "(", "0", ",", "-", "n", ",", "-", "1", ")", ",", "it", ")", "]", "if", "not", "result", ":", "return", "result", "heapify", "(", "result", ")", "top", "=", "result", "[", "0", "]", "[", "0", "]", "order", "=", "-", "n", "_heapreplace", "=", "heapreplace", "for", "elem", "in", "it", ":", "if", "top", "<", "elem", ":", "_heapreplace", "(", "result", ",", "(", "elem", ",", "order", ")", ")", "top", "=", "result", "[", "0", "]", "[", "0", "]", "order", "-=", "1", "result", ".", "sort", "(", "reverse", "=", "True", ")", "return", "[", "r", "[", "0", "]", "for", "r", "in", "result", "]", "# General case, slowest method", "it", "=", "iter", "(", "iterable", ")", "result", "=", "[", "(", "key", "(", "elem", ")", ",", "i", ",", "elem", ")", "for", "i", ",", "elem", "in", "zip", "(", "range", "(", "0", ",", "-", "n", ",", "-", "1", ")", ",", "it", ")", "]", "if", "not", "result", ":", "return", "result", "heapify", "(", "result", ")", "top", "=", "result", "[", "0", "]", "[", "0", "]", "order", "=", "-", "n", "_heapreplace", "=", "heapreplace", "for", "elem", "in", "it", ":", "k", "=", "key", "(", "elem", ")", "if", "top", "<", "k", ":", "_heapreplace", "(", "result", ",", "(", "k", ",", "order", ",", "elem", ")", ")", "top", "=", "result", "[", "0", "]", "[", "0", "]", "order", "-=", "1", "result", ".", "sort", "(", "reverse", "=", "True", ")", "return", "[", "r", "[", "2", "]", "for", "r", "in", "result", "]"], "docstring": "Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]", "docstring_tokens": ["Find", "the", "n", "largest", "elements", "in", "a", "dataset", "."], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/heapq3.py#L805-L864", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/arrays/interval.py", "func_name": "maybe_convert_platform_interval", "original_string": "def maybe_convert_platform_interval(values):\n    \"\"\"\n    Try to do platform conversion, with special casing for IntervalArray.\n    Wrapper around maybe_convert_platform that alters the default return\n    dtype in certain cases to be compatible with IntervalArray.  For example,\n    empty lists return with integer dtype instead of object dtype, which is\n    prohibited for IntervalArray.\n\n    Parameters\n    ----------\n    values : array-like\n\n    Returns\n    -------\n    array\n    \"\"\"\n    if isinstance(values, (list, tuple)) and len(values) == 0:\n        # GH 19016\n        # empty lists/tuples get object dtype by default, but this is not\n        # prohibited for IntervalArray, so coerce to integer instead\n        return np.array([], dtype=np.int64)\n    elif is_categorical_dtype(values):\n        values = np.asarray(values)\n\n    return maybe_convert_platform(values)", "language": "python", "code": "def maybe_convert_platform_interval(values):\n    \"\"\"\n    Try to do platform conversion, with special casing for IntervalArray.\n    Wrapper around maybe_convert_platform that alters the default return\n    dtype in certain cases to be compatible with IntervalArray.  For example,\n    empty lists return with integer dtype instead of object dtype, which is\n    prohibited for IntervalArray.\n\n    Parameters\n    ----------\n    values : array-like\n\n    Returns\n    -------\n    array\n    \"\"\"\n    if isinstance(values, (list, tuple)) and len(values) == 0:\n        # GH 19016\n        # empty lists/tuples get object dtype by default, but this is not\n        # prohibited for IntervalArray, so coerce to integer instead\n        return np.array([], dtype=np.int64)\n    elif is_categorical_dtype(values):\n        values = np.asarray(values)\n\n    return maybe_convert_platform(values)", "code_tokens": ["def", "maybe_convert_platform_interval", "(", "values", ")", ":", "if", "isinstance", "(", "values", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "values", ")", "==", "0", ":", "# GH 19016", "# empty lists/tuples get object dtype by default, but this is not", "# prohibited for IntervalArray, so coerce to integer instead", "return", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "np", ".", "int64", ")", "elif", "is_categorical_dtype", "(", "values", ")", ":", "values", "=", "np", ".", "asarray", "(", "values", ")", "return", "maybe_convert_platform", "(", "values", ")"], "docstring": "Try to do platform conversion, with special casing for IntervalArray.\n    Wrapper around maybe_convert_platform that alters the default return\n    dtype in certain cases to be compatible with IntervalArray.  For example,\n    empty lists return with integer dtype instead of object dtype, which is\n    prohibited for IntervalArray.\n\n    Parameters\n    ----------\n    values : array-like\n\n    Returns\n    -------\n    array", "docstring_tokens": ["Try", "to", "do", "platform", "conversion", "with", "special", "casing", "for", "IntervalArray", ".", "Wrapper", "around", "maybe_convert_platform", "that", "alters", "the", "default", "return", "dtype", "in", "certain", "cases", "to", "be", "compatible", "with", "IntervalArray", ".", "For", "example", "empty", "lists", "return", "with", "integer", "dtype", "instead", "of", "object", "dtype", "which", "is", "prohibited", "for", "IntervalArray", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/arrays/interval.py#L1078-L1102", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/inference.py", "func_name": "is_file_like", "original_string": "def is_file_like(obj):\n    \"\"\"\n    Check if the object is a file-like object.\n\n    For objects to be considered file-like, they must\n    be an iterator AND have either a `read` and/or `write`\n    method as an attribute.\n\n    Note: file-like objects must be iterable, but\n    iterable objects need not be file-like.\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_file_like : bool\n        Whether `obj` has file-like properties.\n\n    Examples\n    --------\n    >>> buffer(StringIO(\"data\"))\n    >>> is_file_like(buffer)\n    True\n    >>> is_file_like([1, 2, 3])\n    False\n    \"\"\"\n\n    if not (hasattr(obj, 'read') or hasattr(obj, 'write')):\n        return False\n\n    if not hasattr(obj, \"__iter__\"):\n        return False\n\n    return True", "language": "python", "code": "def is_file_like(obj):\n    \"\"\"\n    Check if the object is a file-like object.\n\n    For objects to be considered file-like, they must\n    be an iterator AND have either a `read` and/or `write`\n    method as an attribute.\n\n    Note: file-like objects must be iterable, but\n    iterable objects need not be file-like.\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_file_like : bool\n        Whether `obj` has file-like properties.\n\n    Examples\n    --------\n    >>> buffer(StringIO(\"data\"))\n    >>> is_file_like(buffer)\n    True\n    >>> is_file_like([1, 2, 3])\n    False\n    \"\"\"\n\n    if not (hasattr(obj, 'read') or hasattr(obj, 'write')):\n        return False\n\n    if not hasattr(obj, \"__iter__\"):\n        return False\n\n    return True", "code_tokens": ["def", "is_file_like", "(", "obj", ")", ":", "if", "not", "(", "hasattr", "(", "obj", ",", "'read'", ")", "or", "hasattr", "(", "obj", ",", "'write'", ")", ")", ":", "return", "False", "if", "not", "hasattr", "(", "obj", ",", "\"__iter__\"", ")", ":", "return", "False", "return", "True"], "docstring": "Check if the object is a file-like object.\n\n    For objects to be considered file-like, they must\n    be an iterator AND have either a `read` and/or `write`\n    method as an attribute.\n\n    Note: file-like objects must be iterable, but\n    iterable objects need not be file-like.\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_file_like : bool\n        Whether `obj` has file-like properties.\n\n    Examples\n    --------\n    >>> buffer(StringIO(\"data\"))\n    >>> is_file_like(buffer)\n    True\n    >>> is_file_like([1, 2, 3])\n    False", "docstring_tokens": ["Check", "if", "the", "object", "is", "a", "file", "-", "like", "object", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/inference.py#L152-L189", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/inference.py", "func_name": "is_list_like", "original_string": "def is_list_like(obj, allow_sets=True):\n    \"\"\"\n    Check if the object is list-like.\n\n    Objects that are considered list-like are for example Python\n    lists, tuples, sets, NumPy arrays, and Pandas Series.\n\n    Strings and datetime objects, however, are not considered list-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n    allow_sets : boolean, default True\n        If this parameter is False, sets will not be considered list-like\n\n        .. versionadded:: 0.24.0\n\n    Returns\n    -------\n    is_list_like : bool\n        Whether `obj` has list-like properties.\n\n    Examples\n    --------\n    >>> is_list_like([1, 2, 3])\n    True\n    >>> is_list_like({1, 2, 3})\n    True\n    >>> is_list_like(datetime(2017, 1, 1))\n    False\n    >>> is_list_like(\"foo\")\n    False\n    >>> is_list_like(1)\n    False\n    >>> is_list_like(np.array([2]))\n    True\n    >>> is_list_like(np.array(2)))\n    False\n    \"\"\"\n\n    return (isinstance(obj, abc.Iterable) and\n            # we do not count strings/unicode/bytes as list-like\n            not isinstance(obj, (str, bytes)) and\n\n            # exclude zero-dimensional numpy arrays, effectively scalars\n            not (isinstance(obj, np.ndarray) and obj.ndim == 0) and\n\n            # exclude sets if allow_sets is False\n            not (allow_sets is False and isinstance(obj, abc.Set)))", "language": "python", "code": "def is_list_like(obj, allow_sets=True):\n    \"\"\"\n    Check if the object is list-like.\n\n    Objects that are considered list-like are for example Python\n    lists, tuples, sets, NumPy arrays, and Pandas Series.\n\n    Strings and datetime objects, however, are not considered list-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n    allow_sets : boolean, default True\n        If this parameter is False, sets will not be considered list-like\n\n        .. versionadded:: 0.24.0\n\n    Returns\n    -------\n    is_list_like : bool\n        Whether `obj` has list-like properties.\n\n    Examples\n    --------\n    >>> is_list_like([1, 2, 3])\n    True\n    >>> is_list_like({1, 2, 3})\n    True\n    >>> is_list_like(datetime(2017, 1, 1))\n    False\n    >>> is_list_like(\"foo\")\n    False\n    >>> is_list_like(1)\n    False\n    >>> is_list_like(np.array([2]))\n    True\n    >>> is_list_like(np.array(2)))\n    False\n    \"\"\"\n\n    return (isinstance(obj, abc.Iterable) and\n            # we do not count strings/unicode/bytes as list-like\n            not isinstance(obj, (str, bytes)) and\n\n            # exclude zero-dimensional numpy arrays, effectively scalars\n            not (isinstance(obj, np.ndarray) and obj.ndim == 0) and\n\n            # exclude sets if allow_sets is False\n            not (allow_sets is False and isinstance(obj, abc.Set)))", "code_tokens": ["def", "is_list_like", "(", "obj", ",", "allow_sets", "=", "True", ")", ":", "return", "(", "isinstance", "(", "obj", ",", "abc", ".", "Iterable", ")", "and", "# we do not count strings/unicode/bytes as list-like", "not", "isinstance", "(", "obj", ",", "(", "str", ",", "bytes", ")", ")", "and", "# exclude zero-dimensional numpy arrays, effectively scalars", "not", "(", "isinstance", "(", "obj", ",", "np", ".", "ndarray", ")", "and", "obj", ".", "ndim", "==", "0", ")", "and", "# exclude sets if allow_sets is False", "not", "(", "allow_sets", "is", "False", "and", "isinstance", "(", "obj", ",", "abc", ".", "Set", ")", ")", ")"], "docstring": "Check if the object is list-like.\n\n    Objects that are considered list-like are for example Python\n    lists, tuples, sets, NumPy arrays, and Pandas Series.\n\n    Strings and datetime objects, however, are not considered list-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n    allow_sets : boolean, default True\n        If this parameter is False, sets will not be considered list-like\n\n        .. versionadded:: 0.24.0\n\n    Returns\n    -------\n    is_list_like : bool\n        Whether `obj` has list-like properties.\n\n    Examples\n    --------\n    >>> is_list_like([1, 2, 3])\n    True\n    >>> is_list_like({1, 2, 3})\n    True\n    >>> is_list_like(datetime(2017, 1, 1))\n    False\n    >>> is_list_like(\"foo\")\n    False\n    >>> is_list_like(1)\n    False\n    >>> is_list_like(np.array([2]))\n    True\n    >>> is_list_like(np.array(2)))\n    False", "docstring_tokens": ["Check", "if", "the", "object", "is", "list", "-", "like", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/inference.py#L245-L293", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/inference.py", "func_name": "is_nested_list_like", "original_string": "def is_nested_list_like(obj):\n    \"\"\"\n    Check if the object is list-like, and that all of its elements\n    are also list-like.\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_list_like : bool\n        Whether `obj` has list-like properties.\n\n    Examples\n    --------\n    >>> is_nested_list_like([[1, 2, 3]])\n    True\n    >>> is_nested_list_like([{1, 2, 3}, {1, 2, 3}])\n    True\n    >>> is_nested_list_like([\"foo\"])\n    False\n    >>> is_nested_list_like([])\n    False\n    >>> is_nested_list_like([[1, 2, 3], 1])\n    False\n\n    Notes\n    -----\n    This won't reliably detect whether a consumable iterator (e. g.\n    a generator) is a nested-list-like without consuming the iterator.\n    To avoid consuming it, we always return False if the outer container\n    doesn't define `__len__`.\n\n    See Also\n    --------\n    is_list_like\n    \"\"\"\n    return (is_list_like(obj) and hasattr(obj, '__len__') and\n            len(obj) > 0 and all(is_list_like(item) for item in obj))", "language": "python", "code": "def is_nested_list_like(obj):\n    \"\"\"\n    Check if the object is list-like, and that all of its elements\n    are also list-like.\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_list_like : bool\n        Whether `obj` has list-like properties.\n\n    Examples\n    --------\n    >>> is_nested_list_like([[1, 2, 3]])\n    True\n    >>> is_nested_list_like([{1, 2, 3}, {1, 2, 3}])\n    True\n    >>> is_nested_list_like([\"foo\"])\n    False\n    >>> is_nested_list_like([])\n    False\n    >>> is_nested_list_like([[1, 2, 3], 1])\n    False\n\n    Notes\n    -----\n    This won't reliably detect whether a consumable iterator (e. g.\n    a generator) is a nested-list-like without consuming the iterator.\n    To avoid consuming it, we always return False if the outer container\n    doesn't define `__len__`.\n\n    See Also\n    --------\n    is_list_like\n    \"\"\"\n    return (is_list_like(obj) and hasattr(obj, '__len__') and\n            len(obj) > 0 and all(is_list_like(item) for item in obj))", "code_tokens": ["def", "is_nested_list_like", "(", "obj", ")", ":", "return", "(", "is_list_like", "(", "obj", ")", "and", "hasattr", "(", "obj", ",", "'__len__'", ")", "and", "len", "(", "obj", ")", ">", "0", "and", "all", "(", "is_list_like", "(", "item", ")", "for", "item", "in", "obj", ")", ")"], "docstring": "Check if the object is list-like, and that all of its elements\n    are also list-like.\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_list_like : bool\n        Whether `obj` has list-like properties.\n\n    Examples\n    --------\n    >>> is_nested_list_like([[1, 2, 3]])\n    True\n    >>> is_nested_list_like([{1, 2, 3}, {1, 2, 3}])\n    True\n    >>> is_nested_list_like([\"foo\"])\n    False\n    >>> is_nested_list_like([])\n    False\n    >>> is_nested_list_like([[1, 2, 3], 1])\n    False\n\n    Notes\n    -----\n    This won't reliably detect whether a consumable iterator (e. g.\n    a generator) is a nested-list-like without consuming the iterator.\n    To avoid consuming it, we always return False if the outer container\n    doesn't define `__len__`.\n\n    See Also\n    --------\n    is_list_like", "docstring_tokens": ["Check", "if", "the", "object", "is", "list", "-", "like", "and", "that", "all", "of", "its", "elements", "are", "also", "list", "-", "like", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/inference.py#L329-L370", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/inference.py", "func_name": "is_dict_like", "original_string": "def is_dict_like(obj):\n    \"\"\"\n    Check if the object is dict-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_dict_like : bool\n        Whether `obj` has dict-like properties.\n\n    Examples\n    --------\n    >>> is_dict_like({1: 2})\n    True\n    >>> is_dict_like([1, 2, 3])\n    False\n    >>> is_dict_like(dict)\n    False\n    >>> is_dict_like(dict())\n    True\n    \"\"\"\n    dict_like_attrs = (\"__getitem__\", \"keys\", \"__contains__\")\n    return (all(hasattr(obj, attr) for attr in dict_like_attrs)\n            # [GH 25196] exclude classes\n            and not isinstance(obj, type))", "language": "python", "code": "def is_dict_like(obj):\n    \"\"\"\n    Check if the object is dict-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_dict_like : bool\n        Whether `obj` has dict-like properties.\n\n    Examples\n    --------\n    >>> is_dict_like({1: 2})\n    True\n    >>> is_dict_like([1, 2, 3])\n    False\n    >>> is_dict_like(dict)\n    False\n    >>> is_dict_like(dict())\n    True\n    \"\"\"\n    dict_like_attrs = (\"__getitem__\", \"keys\", \"__contains__\")\n    return (all(hasattr(obj, attr) for attr in dict_like_attrs)\n            # [GH 25196] exclude classes\n            and not isinstance(obj, type))", "code_tokens": ["def", "is_dict_like", "(", "obj", ")", ":", "dict_like_attrs", "=", "(", "\"__getitem__\"", ",", "\"keys\"", ",", "\"__contains__\"", ")", "return", "(", "all", "(", "hasattr", "(", "obj", ",", "attr", ")", "for", "attr", "in", "dict_like_attrs", ")", "# [GH 25196] exclude classes", "and", "not", "isinstance", "(", "obj", ",", "type", ")", ")"], "docstring": "Check if the object is dict-like.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_dict_like : bool\n        Whether `obj` has dict-like properties.\n\n    Examples\n    --------\n    >>> is_dict_like({1: 2})\n    True\n    >>> is_dict_like([1, 2, 3])\n    False\n    >>> is_dict_like(dict)\n    False\n    >>> is_dict_like(dict())\n    True", "docstring_tokens": ["Check", "if", "the", "object", "is", "dict", "-", "like", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/inference.py#L373-L400", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/inference.py", "func_name": "is_sequence", "original_string": "def is_sequence(obj):\n    \"\"\"\n    Check if the object is a sequence of objects.\n    String types are not included as sequences here.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_sequence : bool\n        Whether `obj` is a sequence of objects.\n\n    Examples\n    --------\n    >>> l = [1, 2, 3]\n    >>>\n    >>> is_sequence(l)\n    True\n    >>> is_sequence(iter(l))\n    False\n    \"\"\"\n\n    try:\n        iter(obj)  # Can iterate over it.\n        len(obj)   # Has a length associated with it.\n        return not isinstance(obj, (str, bytes))\n    except (TypeError, AttributeError):\n        return False", "language": "python", "code": "def is_sequence(obj):\n    \"\"\"\n    Check if the object is a sequence of objects.\n    String types are not included as sequences here.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_sequence : bool\n        Whether `obj` is a sequence of objects.\n\n    Examples\n    --------\n    >>> l = [1, 2, 3]\n    >>>\n    >>> is_sequence(l)\n    True\n    >>> is_sequence(iter(l))\n    False\n    \"\"\"\n\n    try:\n        iter(obj)  # Can iterate over it.\n        len(obj)   # Has a length associated with it.\n        return not isinstance(obj, (str, bytes))\n    except (TypeError, AttributeError):\n        return False", "code_tokens": ["def", "is_sequence", "(", "obj", ")", ":", "try", ":", "iter", "(", "obj", ")", "# Can iterate over it.", "len", "(", "obj", ")", "# Has a length associated with it.", "return", "not", "isinstance", "(", "obj", ",", "(", "str", ",", "bytes", ")", ")", "except", "(", "TypeError", ",", "AttributeError", ")", ":", "return", "False"], "docstring": "Check if the object is a sequence of objects.\n    String types are not included as sequences here.\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_sequence : bool\n        Whether `obj` is a sequence of objects.\n\n    Examples\n    --------\n    >>> l = [1, 2, 3]\n    >>>\n    >>> is_sequence(l)\n    True\n    >>> is_sequence(iter(l))\n    False", "docstring_tokens": ["Check", "if", "the", "object", "is", "a", "sequence", "of", "objects", ".", "String", "types", "are", "not", "included", "as", "sequences", "here", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/inference.py#L462-L491", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/cast.py", "func_name": "cast_scalar_to_array", "original_string": "def cast_scalar_to_array(shape, value, dtype=None):\n    \"\"\"\n    create np.ndarray of specified shape and dtype, filled with values\n\n    Parameters\n    ----------\n    shape : tuple\n    value : scalar value\n    dtype : np.dtype, optional\n        dtype to coerce\n\n    Returns\n    -------\n    ndarray of shape, filled with value, of specified / inferred dtype\n\n    \"\"\"\n\n    if dtype is None:\n        dtype, fill_value = infer_dtype_from_scalar(value)\n    else:\n        fill_value = value\n\n    values = np.empty(shape, dtype=dtype)\n    values.fill(fill_value)\n\n    return values", "language": "python", "code": "def cast_scalar_to_array(shape, value, dtype=None):\n    \"\"\"\n    create np.ndarray of specified shape and dtype, filled with values\n\n    Parameters\n    ----------\n    shape : tuple\n    value : scalar value\n    dtype : np.dtype, optional\n        dtype to coerce\n\n    Returns\n    -------\n    ndarray of shape, filled with value, of specified / inferred dtype\n\n    \"\"\"\n\n    if dtype is None:\n        dtype, fill_value = infer_dtype_from_scalar(value)\n    else:\n        fill_value = value\n\n    values = np.empty(shape, dtype=dtype)\n    values.fill(fill_value)\n\n    return values", "code_tokens": ["def", "cast_scalar_to_array", "(", "shape", ",", "value", ",", "dtype", "=", "None", ")", ":", "if", "dtype", "is", "None", ":", "dtype", ",", "fill_value", "=", "infer_dtype_from_scalar", "(", "value", ")", "else", ":", "fill_value", "=", "value", "values", "=", "np", ".", "empty", "(", "shape", ",", "dtype", "=", "dtype", ")", "values", ".", "fill", "(", "fill_value", ")", "return", "values"], "docstring": "create np.ndarray of specified shape and dtype, filled with values\n\n    Parameters\n    ----------\n    shape : tuple\n    value : scalar value\n    dtype : np.dtype, optional\n        dtype to coerce\n\n    Returns\n    -------\n    ndarray of shape, filled with value, of specified / inferred dtype", "docstring_tokens": ["create", "np", ".", "ndarray", "of", "specified", "shape", "and", "dtype", "filled", "with", "values"], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/cast.py#L1132-L1157", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/cast.py", "func_name": "construct_1d_arraylike_from_scalar", "original_string": "def construct_1d_arraylike_from_scalar(value, length, dtype):\n    \"\"\"\n    create a np.ndarray / pandas type of specified shape and dtype\n    filled with values\n\n    Parameters\n    ----------\n    value : scalar value\n    length : int\n    dtype : pandas_dtype / np.dtype\n\n    Returns\n    -------\n    np.ndarray / pandas type of length, filled with value\n\n    \"\"\"\n    if is_datetime64tz_dtype(dtype):\n        from pandas import DatetimeIndex\n        subarr = DatetimeIndex([value] * length, dtype=dtype)\n    elif is_categorical_dtype(dtype):\n        from pandas import Categorical\n        subarr = Categorical([value] * length, dtype=dtype)\n    else:\n        if not isinstance(dtype, (np.dtype, type(np.dtype))):\n            dtype = dtype.dtype\n\n        if length and is_integer_dtype(dtype) and isna(value):\n            # coerce if we have nan for an integer dtype\n            dtype = np.dtype('float64')\n        elif isinstance(dtype, np.dtype) and dtype.kind in (\"U\", \"S\"):\n            # we need to coerce to object dtype to avoid\n            # to allow numpy to take our string as a scalar value\n            dtype = object\n            if not isna(value):\n                value = to_str(value)\n\n        subarr = np.empty(length, dtype=dtype)\n        subarr.fill(value)\n\n    return subarr", "language": "python", "code": "def construct_1d_arraylike_from_scalar(value, length, dtype):\n    \"\"\"\n    create a np.ndarray / pandas type of specified shape and dtype\n    filled with values\n\n    Parameters\n    ----------\n    value : scalar value\n    length : int\n    dtype : pandas_dtype / np.dtype\n\n    Returns\n    -------\n    np.ndarray / pandas type of length, filled with value\n\n    \"\"\"\n    if is_datetime64tz_dtype(dtype):\n        from pandas import DatetimeIndex\n        subarr = DatetimeIndex([value] * length, dtype=dtype)\n    elif is_categorical_dtype(dtype):\n        from pandas import Categorical\n        subarr = Categorical([value] * length, dtype=dtype)\n    else:\n        if not isinstance(dtype, (np.dtype, type(np.dtype))):\n            dtype = dtype.dtype\n\n        if length and is_integer_dtype(dtype) and isna(value):\n            # coerce if we have nan for an integer dtype\n            dtype = np.dtype('float64')\n        elif isinstance(dtype, np.dtype) and dtype.kind in (\"U\", \"S\"):\n            # we need to coerce to object dtype to avoid\n            # to allow numpy to take our string as a scalar value\n            dtype = object\n            if not isna(value):\n                value = to_str(value)\n\n        subarr = np.empty(length, dtype=dtype)\n        subarr.fill(value)\n\n    return subarr", "code_tokens": ["def", "construct_1d_arraylike_from_scalar", "(", "value", ",", "length", ",", "dtype", ")", ":", "if", "is_datetime64tz_dtype", "(", "dtype", ")", ":", "from", "pandas", "import", "DatetimeIndex", "subarr", "=", "DatetimeIndex", "(", "[", "value", "]", "*", "length", ",", "dtype", "=", "dtype", ")", "elif", "is_categorical_dtype", "(", "dtype", ")", ":", "from", "pandas", "import", "Categorical", "subarr", "=", "Categorical", "(", "[", "value", "]", "*", "length", ",", "dtype", "=", "dtype", ")", "else", ":", "if", "not", "isinstance", "(", "dtype", ",", "(", "np", ".", "dtype", ",", "type", "(", "np", ".", "dtype", ")", ")", ")", ":", "dtype", "=", "dtype", ".", "dtype", "if", "length", "and", "is_integer_dtype", "(", "dtype", ")", "and", "isna", "(", "value", ")", ":", "# coerce if we have nan for an integer dtype", "dtype", "=", "np", ".", "dtype", "(", "'float64'", ")", "elif", "isinstance", "(", "dtype", ",", "np", ".", "dtype", ")", "and", "dtype", ".", "kind", "in", "(", "\"U\"", ",", "\"S\"", ")", ":", "# we need to coerce to object dtype to avoid", "# to allow numpy to take our string as a scalar value", "dtype", "=", "object", "if", "not", "isna", "(", "value", ")", ":", "value", "=", "to_str", "(", "value", ")", "subarr", "=", "np", ".", "empty", "(", "length", ",", "dtype", "=", "dtype", ")", "subarr", ".", "fill", "(", "value", ")", "return", "subarr"], "docstring": "create a np.ndarray / pandas type of specified shape and dtype\n    filled with values\n\n    Parameters\n    ----------\n    value : scalar value\n    length : int\n    dtype : pandas_dtype / np.dtype\n\n    Returns\n    -------\n    np.ndarray / pandas type of length, filled with value", "docstring_tokens": ["create", "a", "np", ".", "ndarray", "/", "pandas", "type", "of", "specified", "shape", "and", "dtype", "filled", "with", "values"], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/cast.py#L1160-L1199", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/cast.py", "func_name": "construct_1d_object_array_from_listlike", "original_string": "def construct_1d_object_array_from_listlike(values):\n    \"\"\"\n    Transform any list-like object in a 1-dimensional numpy array of object\n    dtype.\n\n    Parameters\n    ----------\n    values : any iterable which has a len()\n\n    Raises\n    ------\n    TypeError\n        * If `values` does not have a len()\n\n    Returns\n    -------\n    1-dimensional numpy array of dtype object\n    \"\"\"\n    # numpy will try to interpret nested lists as further dimensions, hence\n    # making a 1D array that contains list-likes is a bit tricky:\n    result = np.empty(len(values), dtype='object')\n    result[:] = values\n    return result", "language": "python", "code": "def construct_1d_object_array_from_listlike(values):\n    \"\"\"\n    Transform any list-like object in a 1-dimensional numpy array of object\n    dtype.\n\n    Parameters\n    ----------\n    values : any iterable which has a len()\n\n    Raises\n    ------\n    TypeError\n        * If `values` does not have a len()\n\n    Returns\n    -------\n    1-dimensional numpy array of dtype object\n    \"\"\"\n    # numpy will try to interpret nested lists as further dimensions, hence\n    # making a 1D array that contains list-likes is a bit tricky:\n    result = np.empty(len(values), dtype='object')\n    result[:] = values\n    return result", "code_tokens": ["def", "construct_1d_object_array_from_listlike", "(", "values", ")", ":", "# numpy will try to interpret nested lists as further dimensions, hence", "# making a 1D array that contains list-likes is a bit tricky:", "result", "=", "np", ".", "empty", "(", "len", "(", "values", ")", ",", "dtype", "=", "'object'", ")", "result", "[", ":", "]", "=", "values", "return", "result"], "docstring": "Transform any list-like object in a 1-dimensional numpy array of object\n    dtype.\n\n    Parameters\n    ----------\n    values : any iterable which has a len()\n\n    Raises\n    ------\n    TypeError\n        * If `values` does not have a len()\n\n    Returns\n    -------\n    1-dimensional numpy array of dtype object", "docstring_tokens": ["Transform", "any", "list", "-", "like", "object", "in", "a", "1", "-", "dimensional", "numpy", "array", "of", "object", "dtype", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/cast.py#L1202-L1224", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/cast.py", "func_name": "construct_1d_ndarray_preserving_na", "original_string": "def construct_1d_ndarray_preserving_na(values, dtype=None, copy=False):\n    \"\"\"\n    Construct a new ndarray, coercing `values` to `dtype`, preserving NA.\n\n    Parameters\n    ----------\n    values : Sequence\n    dtype : numpy.dtype, optional\n    copy : bool, default False\n        Note that copies may still be made with ``copy=False`` if casting\n        is required.\n\n    Returns\n    -------\n    arr : ndarray[dtype]\n\n    Examples\n    --------\n    >>> np.array([1.0, 2.0, None], dtype='str')\n    array(['1.0', '2.0', 'None'], dtype='<U4')\n\n    >>> construct_1d_ndarray_preserving_na([1.0, 2.0, None], dtype='str')\n\n\n    \"\"\"\n    subarr = np.array(values, dtype=dtype, copy=copy)\n\n    if dtype is not None and dtype.kind in (\"U\", \"S\"):\n        # GH-21083\n        # We can't just return np.array(subarr, dtype='str') since\n        # NumPy will convert the non-string objects into strings\n        # Including NA values. Se we have to go\n        # string -> object -> update NA, which requires an\n        # additional pass over the data.\n        na_values = isna(values)\n        subarr2 = subarr.astype(object)\n        subarr2[na_values] = np.asarray(values, dtype=object)[na_values]\n        subarr = subarr2\n\n    return subarr", "language": "python", "code": "def construct_1d_ndarray_preserving_na(values, dtype=None, copy=False):\n    \"\"\"\n    Construct a new ndarray, coercing `values` to `dtype`, preserving NA.\n\n    Parameters\n    ----------\n    values : Sequence\n    dtype : numpy.dtype, optional\n    copy : bool, default False\n        Note that copies may still be made with ``copy=False`` if casting\n        is required.\n\n    Returns\n    -------\n    arr : ndarray[dtype]\n\n    Examples\n    --------\n    >>> np.array([1.0, 2.0, None], dtype='str')\n    array(['1.0', '2.0', 'None'], dtype='<U4')\n\n    >>> construct_1d_ndarray_preserving_na([1.0, 2.0, None], dtype='str')\n\n\n    \"\"\"\n    subarr = np.array(values, dtype=dtype, copy=copy)\n\n    if dtype is not None and dtype.kind in (\"U\", \"S\"):\n        # GH-21083\n        # We can't just return np.array(subarr, dtype='str') since\n        # NumPy will convert the non-string objects into strings\n        # Including NA values. Se we have to go\n        # string -> object -> update NA, which requires an\n        # additional pass over the data.\n        na_values = isna(values)\n        subarr2 = subarr.astype(object)\n        subarr2[na_values] = np.asarray(values, dtype=object)[na_values]\n        subarr = subarr2\n\n    return subarr", "code_tokens": ["def", "construct_1d_ndarray_preserving_na", "(", "values", ",", "dtype", "=", "None", ",", "copy", "=", "False", ")", ":", "subarr", "=", "np", ".", "array", "(", "values", ",", "dtype", "=", "dtype", ",", "copy", "=", "copy", ")", "if", "dtype", "is", "not", "None", "and", "dtype", ".", "kind", "in", "(", "\"U\"", ",", "\"S\"", ")", ":", "# GH-21083", "# We can't just return np.array(subarr, dtype='str') since", "# NumPy will convert the non-string objects into strings", "# Including NA values. Se we have to go", "# string -> object -> update NA, which requires an", "# additional pass over the data.", "na_values", "=", "isna", "(", "values", ")", "subarr2", "=", "subarr", ".", "astype", "(", "object", ")", "subarr2", "[", "na_values", "]", "=", "np", ".", "asarray", "(", "values", ",", "dtype", "=", "object", ")", "[", "na_values", "]", "subarr", "=", "subarr2", "return", "subarr"], "docstring": "Construct a new ndarray, coercing `values` to `dtype`, preserving NA.\n\n    Parameters\n    ----------\n    values : Sequence\n    dtype : numpy.dtype, optional\n    copy : bool, default False\n        Note that copies may still be made with ``copy=False`` if casting\n        is required.\n\n    Returns\n    -------\n    arr : ndarray[dtype]\n\n    Examples\n    --------\n    >>> np.array([1.0, 2.0, None], dtype='str')\n    array(['1.0', '2.0', 'None'], dtype='<U4')\n\n    >>> construct_1d_ndarray_preserving_na([1.0, 2.0, None], dtype='str')", "docstring_tokens": ["Construct", "a", "new", "ndarray", "coercing", "values", "to", "dtype", "preserving", "NA", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/cast.py#L1227-L1266", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/core/dtypes/cast.py", "func_name": "maybe_cast_to_integer_array", "original_string": "def maybe_cast_to_integer_array(arr, dtype, copy=False):\n    \"\"\"\n    Takes any dtype and returns the casted version, raising for when data is\n    incompatible with integer/unsigned integer dtypes.\n\n    .. versionadded:: 0.24.0\n\n    Parameters\n    ----------\n    arr : array-like\n        The array to cast.\n    dtype : str, np.dtype\n        The integer dtype to cast the array to.\n    copy: boolean, default False\n        Whether to make a copy of the array before returning.\n\n    Returns\n    -------\n    int_arr : ndarray\n        An array of integer or unsigned integer dtype\n\n    Raises\n    ------\n    OverflowError : the dtype is incompatible with the data\n    ValueError : loss of precision has occurred during casting\n\n    Examples\n    --------\n    If you try to coerce negative values to unsigned integers, it raises:\n\n    >>> Series([-1], dtype=\"uint64\")\n    Traceback (most recent call last):\n        ...\n    OverflowError: Trying to coerce negative values to unsigned integers\n\n    Also, if you try to coerce float values to integers, it raises:\n\n    >>> Series([1, 2, 3.5], dtype=\"int64\")\n    Traceback (most recent call last):\n        ...\n    ValueError: Trying to coerce float values to integers\n    \"\"\"\n\n    try:\n        if not hasattr(arr, \"astype\"):\n            casted = np.array(arr, dtype=dtype, copy=copy)\n        else:\n            casted = arr.astype(dtype, copy=copy)\n    except OverflowError:\n        raise OverflowError(\"The elements provided in the data cannot all be \"\n                            \"casted to the dtype {dtype}\".format(dtype=dtype))\n\n    if np.array_equal(arr, casted):\n        return casted\n\n    # We do this casting to allow for proper\n    # data and dtype checking.\n    #\n    # We didn't do this earlier because NumPy\n    # doesn't handle `uint64` correctly.\n    arr = np.asarray(arr)\n\n    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n        raise OverflowError(\"Trying to coerce negative values \"\n                            \"to unsigned integers\")\n\n    if is_integer_dtype(dtype) and (is_float_dtype(arr) or\n                                    is_object_dtype(arr)):\n        raise ValueError(\"Trying to coerce float values to integers\")", "language": "python", "code": "def maybe_cast_to_integer_array(arr, dtype, copy=False):\n    \"\"\"\n    Takes any dtype and returns the casted version, raising for when data is\n    incompatible with integer/unsigned integer dtypes.\n\n    .. versionadded:: 0.24.0\n\n    Parameters\n    ----------\n    arr : array-like\n        The array to cast.\n    dtype : str, np.dtype\n        The integer dtype to cast the array to.\n    copy: boolean, default False\n        Whether to make a copy of the array before returning.\n\n    Returns\n    -------\n    int_arr : ndarray\n        An array of integer or unsigned integer dtype\n\n    Raises\n    ------\n    OverflowError : the dtype is incompatible with the data\n    ValueError : loss of precision has occurred during casting\n\n    Examples\n    --------\n    If you try to coerce negative values to unsigned integers, it raises:\n\n    >>> Series([-1], dtype=\"uint64\")\n    Traceback (most recent call last):\n        ...\n    OverflowError: Trying to coerce negative values to unsigned integers\n\n    Also, if you try to coerce float values to integers, it raises:\n\n    >>> Series([1, 2, 3.5], dtype=\"int64\")\n    Traceback (most recent call last):\n        ...\n    ValueError: Trying to coerce float values to integers\n    \"\"\"\n\n    try:\n        if not hasattr(arr, \"astype\"):\n            casted = np.array(arr, dtype=dtype, copy=copy)\n        else:\n            casted = arr.astype(dtype, copy=copy)\n    except OverflowError:\n        raise OverflowError(\"The elements provided in the data cannot all be \"\n                            \"casted to the dtype {dtype}\".format(dtype=dtype))\n\n    if np.array_equal(arr, casted):\n        return casted\n\n    # We do this casting to allow for proper\n    # data and dtype checking.\n    #\n    # We didn't do this earlier because NumPy\n    # doesn't handle `uint64` correctly.\n    arr = np.asarray(arr)\n\n    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n        raise OverflowError(\"Trying to coerce negative values \"\n                            \"to unsigned integers\")\n\n    if is_integer_dtype(dtype) and (is_float_dtype(arr) or\n                                    is_object_dtype(arr)):\n        raise ValueError(\"Trying to coerce float values to integers\")", "code_tokens": ["def", "maybe_cast_to_integer_array", "(", "arr", ",", "dtype", ",", "copy", "=", "False", ")", ":", "try", ":", "if", "not", "hasattr", "(", "arr", ",", "\"astype\"", ")", ":", "casted", "=", "np", ".", "array", "(", "arr", ",", "dtype", "=", "dtype", ",", "copy", "=", "copy", ")", "else", ":", "casted", "=", "arr", ".", "astype", "(", "dtype", ",", "copy", "=", "copy", ")", "except", "OverflowError", ":", "raise", "OverflowError", "(", "\"The elements provided in the data cannot all be \"", "\"casted to the dtype {dtype}\"", ".", "format", "(", "dtype", "=", "dtype", ")", ")", "if", "np", ".", "array_equal", "(", "arr", ",", "casted", ")", ":", "return", "casted", "# We do this casting to allow for proper", "# data and dtype checking.", "#", "# We didn't do this earlier because NumPy", "# doesn't handle `uint64` correctly.", "arr", "=", "np", ".", "asarray", "(", "arr", ")", "if", "is_unsigned_integer_dtype", "(", "dtype", ")", "and", "(", "arr", "<", "0", ")", ".", "any", "(", ")", ":", "raise", "OverflowError", "(", "\"Trying to coerce negative values \"", "\"to unsigned integers\"", ")", "if", "is_integer_dtype", "(", "dtype", ")", "and", "(", "is_float_dtype", "(", "arr", ")", "or", "is_object_dtype", "(", "arr", ")", ")", ":", "raise", "ValueError", "(", "\"Trying to coerce float values to integers\"", ")"], "docstring": "Takes any dtype and returns the casted version, raising for when data is\n    incompatible with integer/unsigned integer dtypes.\n\n    .. versionadded:: 0.24.0\n\n    Parameters\n    ----------\n    arr : array-like\n        The array to cast.\n    dtype : str, np.dtype\n        The integer dtype to cast the array to.\n    copy: boolean, default False\n        Whether to make a copy of the array before returning.\n\n    Returns\n    -------\n    int_arr : ndarray\n        An array of integer or unsigned integer dtype\n\n    Raises\n    ------\n    OverflowError : the dtype is incompatible with the data\n    ValueError : loss of precision has occurred during casting\n\n    Examples\n    --------\n    If you try to coerce negative values to unsigned integers, it raises:\n\n    >>> Series([-1], dtype=\"uint64\")\n    Traceback (most recent call last):\n        ...\n    OverflowError: Trying to coerce negative values to unsigned integers\n\n    Also, if you try to coerce float values to integers, it raises:\n\n    >>> Series([1, 2, 3.5], dtype=\"int64\")\n    Traceback (most recent call last):\n        ...\n    ValueError: Trying to coerce float values to integers", "docstring_tokens": ["Takes", "any", "dtype", "and", "returns", "the", "casted", "version", "raising", "for", "when", "data", "is", "incompatible", "with", "integer", "/", "unsigned", "integer", "dtypes", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/dtypes/cast.py#L1269-L1337", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/plotting/_core.py", "func_name": "scatter_plot", "original_string": "def scatter_plot(data, x, y, by=None, ax=None, figsize=None, grid=False,\n                 **kwargs):\n    \"\"\"\n    Make a scatter plot from two DataFrame columns\n\n    Parameters\n    ----------\n    data : DataFrame\n    x : Column name for the x-axis values\n    y : Column name for the y-axis values\n    ax : Matplotlib axis object\n    figsize : A tuple (width, height) in inches\n    grid : Setting this to True will show the grid\n    kwargs : other plotting keyword arguments\n        To be passed to scatter function\n\n    Returns\n    -------\n    matplotlib.Figure\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    kwargs.setdefault('edgecolors', 'none')\n\n    def plot_group(group, ax):\n        xvals = group[x].values\n        yvals = group[y].values\n        ax.scatter(xvals, yvals, **kwargs)\n        ax.grid(grid)\n\n    if by is not None:\n        fig = _grouped_plot(plot_group, data, by=by, figsize=figsize, ax=ax)\n    else:\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        else:\n            fig = ax.get_figure()\n        plot_group(data, ax)\n        ax.set_ylabel(pprint_thing(y))\n        ax.set_xlabel(pprint_thing(x))\n\n        ax.grid(grid)\n\n    return fig", "language": "python", "code": "def scatter_plot(data, x, y, by=None, ax=None, figsize=None, grid=False,\n                 **kwargs):\n    \"\"\"\n    Make a scatter plot from two DataFrame columns\n\n    Parameters\n    ----------\n    data : DataFrame\n    x : Column name for the x-axis values\n    y : Column name for the y-axis values\n    ax : Matplotlib axis object\n    figsize : A tuple (width, height) in inches\n    grid : Setting this to True will show the grid\n    kwargs : other plotting keyword arguments\n        To be passed to scatter function\n\n    Returns\n    -------\n    matplotlib.Figure\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    kwargs.setdefault('edgecolors', 'none')\n\n    def plot_group(group, ax):\n        xvals = group[x].values\n        yvals = group[y].values\n        ax.scatter(xvals, yvals, **kwargs)\n        ax.grid(grid)\n\n    if by is not None:\n        fig = _grouped_plot(plot_group, data, by=by, figsize=figsize, ax=ax)\n    else:\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        else:\n            fig = ax.get_figure()\n        plot_group(data, ax)\n        ax.set_ylabel(pprint_thing(y))\n        ax.set_xlabel(pprint_thing(x))\n\n        ax.grid(grid)\n\n    return fig", "code_tokens": ["def", "scatter_plot", "(", "data", ",", "x", ",", "y", ",", "by", "=", "None", ",", "ax", "=", "None", ",", "figsize", "=", "None", ",", "grid", "=", "False", ",", "*", "*", "kwargs", ")", ":", "import", "matplotlib", ".", "pyplot", "as", "plt", "kwargs", ".", "setdefault", "(", "'edgecolors'", ",", "'none'", ")", "def", "plot_group", "(", "group", ",", "ax", ")", ":", "xvals", "=", "group", "[", "x", "]", ".", "values", "yvals", "=", "group", "[", "y", "]", ".", "values", "ax", ".", "scatter", "(", "xvals", ",", "yvals", ",", "*", "*", "kwargs", ")", "ax", ".", "grid", "(", "grid", ")", "if", "by", "is", "not", "None", ":", "fig", "=", "_grouped_plot", "(", "plot_group", ",", "data", ",", "by", "=", "by", ",", "figsize", "=", "figsize", ",", "ax", "=", "ax", ")", "else", ":", "if", "ax", "is", "None", ":", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "else", ":", "fig", "=", "ax", ".", "get_figure", "(", ")", "plot_group", "(", "data", ",", "ax", ")", "ax", ".", "set_ylabel", "(", "pprint_thing", "(", "y", ")", ")", "ax", ".", "set_xlabel", "(", "pprint_thing", "(", "x", ")", ")", "ax", ".", "grid", "(", "grid", ")", "return", "fig"], "docstring": "Make a scatter plot from two DataFrame columns\n\n    Parameters\n    ----------\n    data : DataFrame\n    x : Column name for the x-axis values\n    y : Column name for the y-axis values\n    ax : Matplotlib axis object\n    figsize : A tuple (width, height) in inches\n    grid : Setting this to True will show the grid\n    kwargs : other plotting keyword arguments\n        To be passed to scatter function\n\n    Returns\n    -------\n    matplotlib.Figure", "docstring_tokens": ["Make", "a", "scatter", "plot", "from", "two", "DataFrame", "columns"], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/plotting/_core.py#L2284-L2328", "partition": "train"}
,{"repo": "pandas-dev/pandas", "path": "pandas/plotting/_core.py", "func_name": "hist_frame", "original_string": "def hist_frame(data, column=None, by=None, grid=True, xlabelsize=None,\n               xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False,\n               sharey=False, figsize=None, layout=None, bins=10, **kwds):\n    \"\"\"\n    Make a histogram of the DataFrame's.\n\n    A `histogram`_ is a representation of the distribution of data.\n    This function calls :meth:`matplotlib.pyplot.hist`, on each series in\n    the DataFrame, resulting in one histogram per column.\n\n    .. _histogram: https://en.wikipedia.org/wiki/Histogram\n\n    Parameters\n    ----------\n    data : DataFrame\n        The pandas object holding the data.\n    column : string or sequence\n        If passed, will be used to limit data to a subset of columns.\n    by : object, optional\n        If passed, then used to form histograms for separate groups.\n    grid : bool, default True\n        Whether to show axis grid lines.\n    xlabelsize : int, default None\n        If specified changes the x-axis label size.\n    xrot : float, default None\n        Rotation of x axis labels. For example, a value of 90 displays the\n        x labels rotated 90 degrees clockwise.\n    ylabelsize : int, default None\n        If specified changes the y-axis label size.\n    yrot : float, default None\n        Rotation of y axis labels. For example, a value of 90 displays the\n        y labels rotated 90 degrees clockwise.\n    ax : Matplotlib axes object, default None\n        The axes to plot the histogram on.\n    sharex : bool, default True if ax is None else False\n        In case subplots=True, share x axis and set some x axis labels to\n        invisible; defaults to True if ax is None otherwise False if an ax\n        is passed in.\n        Note that passing in both an ax and sharex=True will alter all x axis\n        labels for all subplots in a figure.\n    sharey : bool, default False\n        In case subplots=True, share y axis and set some y axis labels to\n        invisible.\n    figsize : tuple\n        The size in inches of the figure to create. Uses the value in\n        `matplotlib.rcParams` by default.\n    layout : tuple, optional\n        Tuple of (rows, columns) for the layout of the histograms.\n    bins : integer or sequence, default 10\n        Number of histogram bins to be used. If an integer is given, bins + 1\n        bin edges are calculated and returned. If bins is a sequence, gives\n        bin edges, including left edge of first bin and right edge of last\n        bin. In this case, bins is returned unmodified.\n    **kwds\n        All other plotting keyword arguments to be passed to\n        :meth:`matplotlib.pyplot.hist`.\n\n    Returns\n    -------\n    matplotlib.AxesSubplot or numpy.ndarray of them\n\n    See Also\n    --------\n    matplotlib.pyplot.hist : Plot a histogram using matplotlib.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        This example draws a histogram based on the length and width of\n        some animals, displayed in three bins\n\n        >>> df = pd.DataFrame({\n        ...     'length': [1.5, 0.5, 1.2, 0.9, 3],\n        ...     'width': [0.7, 0.2, 0.15, 0.2, 1.1]\n        ...     }, index= ['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n        >>> hist = df.hist(bins=3)\n    \"\"\"\n    _raise_if_no_mpl()\n    _converter._WARN = False\n    if by is not None:\n        axes = grouped_hist(data, column=column, by=by, ax=ax, grid=grid,\n                            figsize=figsize, sharex=sharex, sharey=sharey,\n                            layout=layout, bins=bins, xlabelsize=xlabelsize,\n                            xrot=xrot, ylabelsize=ylabelsize,\n                            yrot=yrot, **kwds)\n        return axes\n\n    if column is not None:\n        if not isinstance(column, (list, np.ndarray, ABCIndexClass)):\n            column = [column]\n        data = data[column]\n    data = data._get_numeric_data()\n    naxes = len(data.columns)\n\n    fig, axes = _subplots(naxes=naxes, ax=ax, squeeze=False,\n                          sharex=sharex, sharey=sharey, figsize=figsize,\n                          layout=layout)\n    _axes = _flatten(axes)\n\n    for i, col in enumerate(com.try_sort(data.columns)):\n        ax = _axes[i]\n        ax.hist(data[col].dropna().values, bins=bins, **kwds)\n        ax.set_title(col)\n        ax.grid(grid)\n\n    _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\n                     ylabelsize=ylabelsize, yrot=yrot)\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\n\n    return axes", "language": "python", "code": "def hist_frame(data, column=None, by=None, grid=True, xlabelsize=None,\n               xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False,\n               sharey=False, figsize=None, layout=None, bins=10, **kwds):\n    \"\"\"\n    Make a histogram of the DataFrame's.\n\n    A `histogram`_ is a representation of the distribution of data.\n    This function calls :meth:`matplotlib.pyplot.hist`, on each series in\n    the DataFrame, resulting in one histogram per column.\n\n    .. _histogram: https://en.wikipedia.org/wiki/Histogram\n\n    Parameters\n    ----------\n    data : DataFrame\n        The pandas object holding the data.\n    column : string or sequence\n        If passed, will be used to limit data to a subset of columns.\n    by : object, optional\n        If passed, then used to form histograms for separate groups.\n    grid : bool, default True\n        Whether to show axis grid lines.\n    xlabelsize : int, default None\n        If specified changes the x-axis label size.\n    xrot : float, default None\n        Rotation of x axis labels. For example, a value of 90 displays the\n        x labels rotated 90 degrees clockwise.\n    ylabelsize : int, default None\n        If specified changes the y-axis label size.\n    yrot : float, default None\n        Rotation of y axis labels. For example, a value of 90 displays the\n        y labels rotated 90 degrees clockwise.\n    ax : Matplotlib axes object, default None\n        The axes to plot the histogram on.\n    sharex : bool, default True if ax is None else False\n        In case subplots=True, share x axis and set some x axis labels to\n        invisible; defaults to True if ax is None otherwise False if an ax\n        is passed in.\n        Note that passing in both an ax and sharex=True will alter all x axis\n        labels for all subplots in a figure.\n    sharey : bool, default False\n        In case subplots=True, share y axis and set some y axis labels to\n        invisible.\n    figsize : tuple\n        The size in inches of the figure to create. Uses the value in\n        `matplotlib.rcParams` by default.\n    layout : tuple, optional\n        Tuple of (rows, columns) for the layout of the histograms.\n    bins : integer or sequence, default 10\n        Number of histogram bins to be used. If an integer is given, bins + 1\n        bin edges are calculated and returned. If bins is a sequence, gives\n        bin edges, including left edge of first bin and right edge of last\n        bin. In this case, bins is returned unmodified.\n    **kwds\n        All other plotting keyword arguments to be passed to\n        :meth:`matplotlib.pyplot.hist`.\n\n    Returns\n    -------\n    matplotlib.AxesSubplot or numpy.ndarray of them\n\n    See Also\n    --------\n    matplotlib.pyplot.hist : Plot a histogram using matplotlib.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        This example draws a histogram based on the length and width of\n        some animals, displayed in three bins\n\n        >>> df = pd.DataFrame({\n        ...     'length': [1.5, 0.5, 1.2, 0.9, 3],\n        ...     'width': [0.7, 0.2, 0.15, 0.2, 1.1]\n        ...     }, index= ['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n        >>> hist = df.hist(bins=3)\n    \"\"\"\n    _raise_if_no_mpl()\n    _converter._WARN = False\n    if by is not None:\n        axes = grouped_hist(data, column=column, by=by, ax=ax, grid=grid,\n                            figsize=figsize, sharex=sharex, sharey=sharey,\n                            layout=layout, bins=bins, xlabelsize=xlabelsize,\n                            xrot=xrot, ylabelsize=ylabelsize,\n                            yrot=yrot, **kwds)\n        return axes\n\n    if column is not None:\n        if not isinstance(column, (list, np.ndarray, ABCIndexClass)):\n            column = [column]\n        data = data[column]\n    data = data._get_numeric_data()\n    naxes = len(data.columns)\n\n    fig, axes = _subplots(naxes=naxes, ax=ax, squeeze=False,\n                          sharex=sharex, sharey=sharey, figsize=figsize,\n                          layout=layout)\n    _axes = _flatten(axes)\n\n    for i, col in enumerate(com.try_sort(data.columns)):\n        ax = _axes[i]\n        ax.hist(data[col].dropna().values, bins=bins, **kwds)\n        ax.set_title(col)\n        ax.grid(grid)\n\n    _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\n                     ylabelsize=ylabelsize, yrot=yrot)\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\n\n    return axes", "code_tokens": ["def", "hist_frame", "(", "data", ",", "column", "=", "None", ",", "by", "=", "None", ",", "grid", "=", "True", ",", "xlabelsize", "=", "None", ",", "xrot", "=", "None", ",", "ylabelsize", "=", "None", ",", "yrot", "=", "None", ",", "ax", "=", "None", ",", "sharex", "=", "False", ",", "sharey", "=", "False", ",", "figsize", "=", "None", ",", "layout", "=", "None", ",", "bins", "=", "10", ",", "*", "*", "kwds", ")", ":", "_raise_if_no_mpl", "(", ")", "_converter", ".", "_WARN", "=", "False", "if", "by", "is", "not", "None", ":", "axes", "=", "grouped_hist", "(", "data", ",", "column", "=", "column", ",", "by", "=", "by", ",", "ax", "=", "ax", ",", "grid", "=", "grid", ",", "figsize", "=", "figsize", ",", "sharex", "=", "sharex", ",", "sharey", "=", "sharey", ",", "layout", "=", "layout", ",", "bins", "=", "bins", ",", "xlabelsize", "=", "xlabelsize", ",", "xrot", "=", "xrot", ",", "ylabelsize", "=", "ylabelsize", ",", "yrot", "=", "yrot", ",", "*", "*", "kwds", ")", "return", "axes", "if", "column", "is", "not", "None", ":", "if", "not", "isinstance", "(", "column", ",", "(", "list", ",", "np", ".", "ndarray", ",", "ABCIndexClass", ")", ")", ":", "column", "=", "[", "column", "]", "data", "=", "data", "[", "column", "]", "data", "=", "data", ".", "_get_numeric_data", "(", ")", "naxes", "=", "len", "(", "data", ".", "columns", ")", "fig", ",", "axes", "=", "_subplots", "(", "naxes", "=", "naxes", ",", "ax", "=", "ax", ",", "squeeze", "=", "False", ",", "sharex", "=", "sharex", ",", "sharey", "=", "sharey", ",", "figsize", "=", "figsize", ",", "layout", "=", "layout", ")", "_axes", "=", "_flatten", "(", "axes", ")", "for", "i", ",", "col", "in", "enumerate", "(", "com", ".", "try_sort", "(", "data", ".", "columns", ")", ")", ":", "ax", "=", "_axes", "[", "i", "]", "ax", ".", "hist", "(", "data", "[", "col", "]", ".", "dropna", "(", ")", ".", "values", ",", "bins", "=", "bins", ",", "*", "*", "kwds", ")", "ax", ".", "set_title", "(", "col", ")", "ax", ".", "grid", "(", "grid", ")", "_set_ticks_props", "(", "axes", ",", "xlabelsize", "=", "xlabelsize", ",", "xrot", "=", "xrot", ",", "ylabelsize", "=", "ylabelsize", ",", "yrot", "=", "yrot", ")", "fig", ".", "subplots_adjust", "(", "wspace", "=", "0.3", ",", "hspace", "=", "0.3", ")", "return", "axes"], "docstring": "Make a histogram of the DataFrame's.\n\n    A `histogram`_ is a representation of the distribution of data.\n    This function calls :meth:`matplotlib.pyplot.hist`, on each series in\n    the DataFrame, resulting in one histogram per column.\n\n    .. _histogram: https://en.wikipedia.org/wiki/Histogram\n\n    Parameters\n    ----------\n    data : DataFrame\n        The pandas object holding the data.\n    column : string or sequence\n        If passed, will be used to limit data to a subset of columns.\n    by : object, optional\n        If passed, then used to form histograms for separate groups.\n    grid : bool, default True\n        Whether to show axis grid lines.\n    xlabelsize : int, default None\n        If specified changes the x-axis label size.\n    xrot : float, default None\n        Rotation of x axis labels. For example, a value of 90 displays the\n        x labels rotated 90 degrees clockwise.\n    ylabelsize : int, default None\n        If specified changes the y-axis label size.\n    yrot : float, default None\n        Rotation of y axis labels. For example, a value of 90 displays the\n        y labels rotated 90 degrees clockwise.\n    ax : Matplotlib axes object, default None\n        The axes to plot the histogram on.\n    sharex : bool, default True if ax is None else False\n        In case subplots=True, share x axis and set some x axis labels to\n        invisible; defaults to True if ax is None otherwise False if an ax\n        is passed in.\n        Note that passing in both an ax and sharex=True will alter all x axis\n        labels for all subplots in a figure.\n    sharey : bool, default False\n        In case subplots=True, share y axis and set some y axis labels to\n        invisible.\n    figsize : tuple\n        The size in inches of the figure to create. Uses the value in\n        `matplotlib.rcParams` by default.\n    layout : tuple, optional\n        Tuple of (rows, columns) for the layout of the histograms.\n    bins : integer or sequence, default 10\n        Number of histogram bins to be used. If an integer is given, bins + 1\n        bin edges are calculated and returned. If bins is a sequence, gives\n        bin edges, including left edge of first bin and right edge of last\n        bin. In this case, bins is returned unmodified.\n    **kwds\n        All other plotting keyword arguments to be passed to\n        :meth:`matplotlib.pyplot.hist`.\n\n    Returns\n    -------\n    matplotlib.AxesSubplot or numpy.ndarray of them\n\n    See Also\n    --------\n    matplotlib.pyplot.hist : Plot a histogram using matplotlib.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        This example draws a histogram based on the length and width of\n        some animals, displayed in three bins\n\n        >>> df = pd.DataFrame({\n        ...     'length': [1.5, 0.5, 1.2, 0.9, 3],\n        ...     'width': [0.7, 0.2, 0.15, 0.2, 1.1]\n        ...     }, index= ['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n        >>> hist = df.hist(bins=3)", "docstring_tokens": ["Make", "a", "histogram", "of", "the", "DataFrame", "s", "."], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/plotting/_core.py#L2331-L2443", "partition": "train"}
]